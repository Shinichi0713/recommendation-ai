RPCAに使われる最適化アルゴリズムである **ADMM (Alternating Direction Method of Multipliers：交互方向乗数法)** について説明します。

RPCA（ロバスト主成分分析）や画像処理、機械学習の最適化で頻繁に登場する ADDM は、線形・凸最適化アルゴリズムの発展系に分類されます。

より専門的なカテゴリで分けるなら、**「近接勾配法（Proximity-based methods）」** や **「分割手法（Splitting methods）」** というグループに属します。

## ADDMが利用される理由

なぜ ADMM が RPCA のような問題でこれほど重宝されるのか、エンジニアの視点でその立ち位置を整理します。

### 1. ADMM の分類上の立ち位置

ADMM は、**「複雑な巨大問題を、解きやすい小さな問題に分割して、交互に解く」** という戦略をとるアルゴリズムです。

* **大分類** : 凸最適化アルゴリズム
* **小分類** : 分割手法（Operator Splitting）、拡張ラグランジュ関数法（Augmented Lagrangian Method）の派生
* **特徴** : 目的関数が「滑らかではない（微分できない）」項を含んでいても、非常に効率的に解くことができます。


### 2. なぜ RPCA で ADMM が使われるのか？

RPCA の目的は、データを「きれいな低ランク行列（**$L$**）」と「外れ値である疎な行列（**$S$**）」に分離することです。

この問題には、数学的に扱いにくい2つの性質があります。

1. **低ランク制約** : 行列のランク（性質）を抑えたい。
2. **疎（Sparse）制約** : ほとんどの要素を 0 にしたい（**$L_1$** ノルム）。

これらを同時に解くのは非常に困難ですが、ADMM を使うと以下のように「分業」ができます。

* **ステップ 1 (**$L$** の更新)** : 特異値分解（SVD）などを用いて「低ランクっぽさ」だけを追求する。
* **ステップ 2 (**$S$** の更新)** : ソフトしきい値演算などを用いて「疎っぽさ」だけを追求する。
* **ステップ 3 (調整)** : 両者のつじつまが合うように調整する。

この **「あっちを立てて、こっちを立てて、最後に帳尻を合わせる」** という交互のプロセスが ADMM の名前（Alternating Direction）の由来です。


### 3. 他のアルゴリズムとの比較

ADMM がなぜ選ばれるのか、他の手法と比較するとそのメリットが際立ちます。

| **アルゴリズム** | **得意なこと**                 | **RPCAにおけるデメリット**                      |
| ---------------------- | ------------------------------------ | ----------------------------------------------------- |
| **勾配降下法**   | 微分可能な関数                       | **$L_1$**ノルムや核ノルムなど、微分できない項に弱い |
| **内点法**       | 厳密な最適解の導出                   | 行列が大きくなると計算量（逆行列等）が爆発する        |
| **ADMM**         | **大規模・分散処理・微分不能** | 収束が（精度を求めすぎると）少し遅い場合がある        |


### 4. エンジニアが ADMM を好む理由：実装の容易さ

ADMM の最大の魅力は、**「並列化・分散化がしやすい」** ことです。

大きな行列をブロックごとに分割して別々のサーバーで計算させ、最後に統合するという処理と非常に相性が良いため、ビッグデータ解析の現場では「とりあえず ADMM で解けないか？」と検討されるほど信頼されています。

## アルゴリズムの着想

ADMM（交互方向乗数法）の着想を一言で言えば、 **「2つの強力な手法の良いとこ取り」** です。

具体的には、 **「デュアル分解（Dual Decomposition）」** の柔軟性と、 **「拡張ラグランジュ関数法（Augmented Lagrangian Method）」** の堅牢性を、エンジニアが使いやすいようにガッチャンコさせたのがADMMです。


### 1. 第1の源泉：デュアル分解（問題をバラバラにしたい）

ADMMの最も根本的な動機は、 **「巨大な問題を、小さなパーツに分けて並列で解きたい」** という点にあります。

例えば、目的関数が $f(x) + g(z)$ のように2つの変数に分かれている場合、デュアル分解という手法を使うと、$x$ と $z$ を別々に計算できるようになります。しかし、この手法には **「関数の形が綺麗（強凸性など）でないと、計算が不安定になりやすい」** という弱点がありました。


### 2. 第2の源泉：拡張ラグランジュ法（計算を安定させたい）

そこで登場するのが、拡張ラグランジュ法です。これは、制約条件を守らせるための「ペナルティ項」を追加することで、計算を非常に安定させる手法です。

しかし、こちらにも弱点がありました。ペナルティ項（2次形式）を追加したせいで、せっかく分かれていた $f(x)$ と $g(z)$ が数式の中で複雑に絡まり合ってしまい、「バラバラに計算する（並列化）」ことができなくなってしまったのです。

### 3. ADMMの着想：交互にやればいいじゃないか！

ここでADMMの画期的な着想が生まれます。

> **「絡まり合って同時に解けないなら、一方を固定して、もう一方を解く。これを交互に繰り返せばいい」**

これが **Alternating Direction（交互方向）** という名前の由来です。

1. $z$ を定数だと思って、$x$ だけを最適化する。
2. 次に $x$ を定数だと思って、$z$ だけを最適化する。
3. 最後に「つじつま合わせ」の変数（デュアル変数）を更新する。

この「交互に解く」というアイデアにより、 **「計算の安定性」を保ったまま、「問題を分割して解く（並列性）」という欲張りな要求を同時に叶えた** のです。


### 4. なぜこれがRPCAなどで「天才的」とされるのか

RPCAのような問題では、目的関数が「低ランク性」と「疎（スパース）性」という、全く性質の異なる2つの要素で構成されています。

* **低ランク担当**：特異値分解（SVD）を使いたい。
* **スパース担当**：しきい値処理（Soft-thresholding）を使いたい。

この **「全く違う専門道具を使いたい2つのパーツ」** を、ADMMなら「交互にそれぞれの専門道具で処理して、最後に統合する」という自然なフローに落とし込めます。この「専門家を個別に呼び出せる」という構造が、現代の複雑なAI最適化に完璧にフィットしたのです。


## アルゴリズム

ADMM（交互方向乗数法）を具体的に解くためのアルゴリズムは、シンプルで規則的です。

数式で見ると難解に思えますが、エンジニアリングの視点では　**「3つの変数を順番にアップデートし続けるループ処理」**　と捉えるのが一番分かりやすいです。

### 1. 最適化したい「基本の形」

ADMMが対象とするのは、次のような「変数が2つに分かれた」問題です。

$$\min_{x, z} f(x) + g(z)$$

$$\text{subject to } Ax + Bz = c$$

ここで、$f(x)$ と $g(z)$ はそれぞれ異なる性質（例えば一方はデータの誤差、もう一方はスパース性など）を持つ関数です。

### 2. ADMMのコア：3ステップ・ループ

ADMMは、以下の3つの更新式を、値が変化しなくなるまで（収束するまで）繰り返します。ここで $u$ は「つじつまを合わせるための調整役（双対変数）」、$\rho$ はステップの大きさを決めるパラメータです。

#### ステップ1：$x$ の更新（$z$ と $u$ を固定して解く）

$$x^{k+1} = \arg\min_{x} \left( f(x) + \frac{\rho}{2} \| Ax + Bz^k - c + u^k \|^2 \right)$$

* **意味**: $z$ と $u$ の値を今のまま固定して、$f(x)$ を最小にする $x$ を探します。

#### ステップ2：$z$ の更新（新しくなった $x$ と $u$ を固定して解く）

$$z^{k+1} = \arg\min_{z} \left( g(z) + \frac{\rho}{2} \| Ax^{k+1} + Bz - c + u^k \|^2 \right)$$

* **意味**: $x$ を最新版に更新し、$g(z)$ を最小にする $z$ を探します。

#### ステップ3：$u$ の更新（エラーを $u$ に蓄積する）

$$u^{k+1} = u^k + (Ax^{k+1} + Bz^{k+1} - c)$$

* **意味**: 「理想的な制約（$Ax + Bz = c$）」からどれだけズレているかを計算し、そのズレを $u$ に反映させます。


### 3. 具体例：Lasso回帰（スパース学習）の場合

ADMMの強力さが一番よくわかるLasso（ラッソ）回帰を例にします。Lassoは「予測誤差を小さくしつつ、重み $z$ をスカスカ（疎）にしたい」という問題です。

1. **$x$ の更新（誤差最小化**:

通常の「最小二乗法」を解く作業です。行列計算だけで一瞬で解けます。
2. **$z$ の更新（スパース化）**:

ここで **「ソフトしきい値演算（Soft-thresholding）」** という専門道具を使います。一定以下の小さな値を 0 に切り捨てる非常に軽い処理です。
3. **uの更新**:

$x$ と $z$ のズレを足し合わせます。

このように、 **「重い行列計算」と「軽い切り捨て処理」を分離して交互に実行できる** のがADMMの真髄です。


### 4. Pythonでの実装イメージ

エンジニアがコードに落とし込む際の構造は、概ね以下のようになります。

```python
def admm(A, b, rho, max_iter):
    # 初期化
    x = np.zeros(n)
    z = np.zeros(n)
    u = np.zeros(n)
    
    for k in range(max_iter):
        # 1. x-update: 最小二乗法的な解き方
        x = solve_least_squares(A, b, z, u, rho)
        
        # 2. z-update: ソフトしきい値演算 (L1ノルムの近接写像)
        z = soft_thresholding(x + u, kappa / rho)
        
        # 3. u-update: 双対変数の更新 (残差の蓄積)
        u = u + (x - z)
        
        # 収束判定
        if np.linalg.norm(x - z) < epsilon:
            break
            
    return z

```

## 総括

ADMMは、特に「ノイズ」と「構造」を分離するタスクで圧倒的な性能を発揮します。

1. RPCA（ロバスト主成分分析）: 監視カメラ映像から背景（低ランク）と動体（スパース）を分離。
2. 画像復元: ぼやけた画像やノイズの乗った画像から元のシャープな画像を推定（TV正則化）。
3. 機械学習（Lasso / SVM）: 数百万の変数から、本当に重要な数個の変数だけを抽出する。
4. 分散最適化: 複数のサーバー間でデータを共有せずに、全体の最適解を共同で計算。
