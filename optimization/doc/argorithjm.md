RPCAに使われる最適化アルゴリズムである **ADMM (Alternating Direction Method of Multipliers：交互方向乗数法)** について説明します。

RPCA（ロバスト主成分分析）や画像処理、機械学習の最適化で頻繁に登場する ADDM は、線形・凸最適化アルゴリズムの発展系に分類されます。

より専門的なカテゴリで分けるなら、**「近接勾配法（Proximity-based methods）」** や **「分割手法（Splitting methods）」** というグループに属します。

## ADDMが利用される理由

なぜ ADMM が RPCA のような問題でこれほど重宝されるのか、エンジニアの視点でその立ち位置を整理します。

### 1. ADMM の分類上の立ち位置

ADMM は、**「複雑な巨大問題を、解きやすい小さな問題に分割して、交互に解く」** という戦略をとるアルゴリズムです。

* **大分類** : 凸最適化アルゴリズム
* **小分類** : 分割手法（Operator Splitting）、拡張ラグランジュ関数法（Augmented Lagrangian Method）の派生
* **特徴** : 目的関数が「滑らかではない（微分できない）」項を含んでいても、非常に効率的に解くことができます。


### 2. なぜ RPCA で ADMM が使われるのか？

RPCA の目的は、データを「きれいな低ランク行列（**$L$**）」と「外れ値である疎な行列（**$S$**）」に分離することです。

この問題には、数学的に扱いにくい2つの性質があります。

1. **低ランク制約** : 行列のランク（性質）を抑えたい。
2. **疎（Sparse）制約** : ほとんどの要素を 0 にしたい（**$L_1$** ノルム）。

これらを同時に解くのは非常に困難ですが、ADMM を使うと以下のように「分業」ができます。

* **ステップ 1 (**$L$** の更新)** : 特異値分解（SVD）などを用いて「低ランクっぽさ」だけを追求する。
* **ステップ 2 (**$S$** の更新)** : ソフトしきい値演算などを用いて「疎っぽさ」だけを追求する。
* **ステップ 3 (調整)** : 両者のつじつまが合うように調整する。

この **「あっちを立てて、こっちを立てて、最後に帳尻を合わせる」** という交互のプロセスが ADMM の名前（Alternating Direction）の由来です。


### 3. 他のアルゴリズムとの比較

ADMM がなぜ選ばれるのか、他の手法と比較するとそのメリットが際立ちます。

| **アルゴリズム** | **得意なこと**                 | **RPCAにおけるデメリット**                      |
| ---------------------- | ------------------------------------ | ----------------------------------------------------- |
| **勾配降下法**   | 微分可能な関数                       | **$L_1$**ノルムや核ノルムなど、微分できない項に弱い |
| **内点法**       | 厳密な最適解の導出                   | 行列が大きくなると計算量（逆行列等）が爆発する        |
| **ADMM**         | **大規模・分散処理・微分不能** | 収束が（精度を求めすぎると）少し遅い場合がある        |


### 4. エンジニアが ADMM を好む理由：実装の容易さ

ADMM の最大の魅力は、**「並列化・分散化がしやすい」** ことです。

大きな行列をブロックごとに分割して別々のサーバーで計算させ、最後に統合するという処理と非常に相性が良いため、ビッグデータ解析の現場では「とりあえず ADMM で解けないか？」と検討されるほど信頼されています。

## アルゴリズムの着想

ADMM（交互方向乗数法）の着想を一言で言えば、 **「2つの強力な手法の良いとこ取り」** です。

具体的には、 **「デュアル分解（Dual Decomposition）」** の柔軟性と、 **「拡張ラグランジュ関数法（Augmented Lagrangian Method）」** の堅牢性を、エンジニアが使いやすいようにガッチャンコさせたのがADMMです。


### 1. 第1の源泉：デュアル分解（問題をバラバラにしたい）

ADMMの最も根本的な動機は、 **「巨大な問題を、小さなパーツに分けて並列で解きたい」** という点にあります。

例えば、目的関数が $f(x) + g(z)$ のように2つの変数に分かれている場合、デュアル分解という手法を使うと、$x$ と $z$ を別々に計算できるようになります。しかし、この手法には **「関数の形が綺麗（強凸性など）でないと、計算が不安定になりやすい」** という弱点がありました。


### 2. 第2の源泉：拡張ラグランジュ法（計算を安定させたい）

そこで登場するのが、拡張ラグランジュ法です。これは、制約条件を守らせるための「ペナルティ項」を追加することで、計算を非常に安定させる手法です。

しかし、こちらにも弱点がありました。ペナルティ項（2次形式）を追加したせいで、せっかく分かれていた $f(x)$ と $g(z)$ が数式の中で複雑に絡まり合ってしまい、「バラバラに計算する（並列化）」ことができなくなってしまったのです。

### 3. ADMMの着想：交互にやればいいじゃないか！

ここでADMMの画期的な着想が生まれます。

> **「絡まり合って同時に解けないなら、一方を固定して、もう一方を解く。これを交互に繰り返せばいい」**

これが **Alternating Direction（交互方向）** という名前の由来です。

1. $z$ を定数だと思って、$x$ だけを最適化する。
2. 次に $x$ を定数だと思って、$z$ だけを最適化する。
3. 最後に「つじつま合わせ」の変数（デュアル変数）を更新する。

この「交互に解く」というアイデアにより、 **「計算の安定性」を保ったまま、「問題を分割して解く（並列性）」という欲張りな要求を同時に叶えた** のです。


### 4. なぜこれがRPCAなどで「天才的」とされるのか

RPCAのような問題では、目的関数が「低ランク性」と「疎（スパース）性」という、全く性質の異なる2つの要素で構成されています。

* **低ランク担当**：特異値分解（SVD）を使いたい。
* **スパース担当**：しきい値処理（Soft-thresholding）を使いたい。

この **「全く違う専門道具を使いたい2つのパーツ」** を、ADMMなら「交互にそれぞれの専門道具で処理して、最後に統合する」という自然なフローに落とし込めます。この「専門家を個別に呼び出せる」という構造が、現代の複雑なAI最適化に完璧にフィットしたのです。


## アルゴリズム

ADMM（交互方向乗数法）を具体的に解くためのアルゴリズムは、シンプルで規則的です。

数式で見ると難解に思えますが、エンジニアリングの視点では　**「3つの変数を順番にアップデートし続けるループ処理」**　と捉えるのが一番分かりやすいです。

### 1. 最適化したい「基本の形」

ADMMが対象とするのは、次のような「変数が2つに分かれた」問題です。

$$\min_{x, z} f(x) + g(z)$$

$$\text{subject to } Ax + Bz = c$$

ここで、$f(x)$ と $g(z)$ はそれぞれ異なる性質（例えば一方はデータの誤差、もう一方はスパース性など）を持つ関数です。

### 2. ADMMのコア：3ステップ・ループ

ADMMは、以下の3つの更新式を、値が変化しなくなるまで（収束するまで）繰り返します。ここで $u$ は「つじつまを合わせるための調整役（双対変数）」、$\rho$ はステップの大きさを決めるパラメータです。

#### ステップ1：$x$ の更新（$z$ と $u$ を固定して解く）

$$x^{k+1} = \arg\min_{x} \left( f(x) + \frac{\rho}{2} \| Ax + Bz^k - c + u^k \|^2 \right)$$

* **意味**: $z$ と $u$ の値を今のまま固定して、$f(x)$ を最小にする $x$ を探します。

#### ステップ2：$z$ の更新（新しくなった $x$ と $u$ を固定して解く）

$$z^{k+1} = \arg\min_{z} \left( g(z) + \frac{\rho}{2} \| Ax^{k+1} + Bz - c + u^k \|^2 \right)$$

* **意味**: $x$ を最新版に更新し、$g(z)$ を最小にする $z$ を探します。

#### ステップ3：$u$ の更新（エラーを $u$ に蓄積する）

$$u^{k+1} = u^k + (Ax^{k+1} + Bz^{k+1} - c)$$

* **意味**: 「理想的な制約（$Ax + Bz = c$）」からどれだけズレているかを計算し、そのズレを $u$ に反映させます。


### 3. 具体例：Lasso回帰（スパース学習）の場合

ADMMの強力さが一番よくわかるLasso（ラッソ）回帰を例にします。Lassoは「予測誤差を小さくしつつ、重み $z$ をスカスカ（疎）にしたい」という問題です。

1. **$x$ の更新（誤差最小化**:

通常の「最小二乗法」を解く作業です。行列計算だけで一瞬で解けます。
2. **$z$ の更新（スパース化）**:

ここで **「ソフトしきい値演算（Soft-thresholding）」** という専門道具を使います。一定以下の小さな値を 0 に切り捨てる非常に軽い処理です。
3. **uの更新**:

$x$ と $z$ のズレを足し合わせます。

このように、 **「重い行列計算」と「軽い切り捨て処理」を分離して交互に実行できる** のがADMMの真髄です。


### 4. Pythonでの実装イメージ

エンジニアがコードに落とし込む際の構造は、概ね以下のようになります。

```python
def admm(A, b, rho, max_iter):
    # 初期化
    x = np.zeros(n)
    z = np.zeros(n)
    u = np.zeros(n)
    
    for k in range(max_iter):
        # 1. x-update: 最小二乗法的な解き方
        x = solve_least_squares(A, b, z, u, rho)
        
        # 2. z-update: ソフトしきい値演算 (L1ノルムの近接写像)
        z = soft_thresholding(x + u, kappa / rho)
        
        # 3. u-update: 双対変数の更新 (残差の蓄積)
        u = u + (x - z)
        
        # 収束判定
        if np.linalg.norm(x - z) < epsilon:
            break
            
    return z

```

## 総括

ADMMは、特に「ノイズ」と「構造」を分離するタスクで圧倒的な性能を発揮します。

1. RPCA（ロバスト主成分分析）: 監視カメラ映像から背景（低ランク）と動体（スパース）を分離。
2. 画像復元: ぼやけた画像やノイズの乗った画像から元のシャープな画像を推定（TV正則化）。
3. 機械学習（Lasso / SVM）: 数百万の変数から、本当に重要な数個の変数だけを抽出する。
4. 分散最適化: 複数のサーバー間でデータを共有せずに、全体の最適解を共同で計算。


## そのほか

最適化アルゴリズムは、山の頂上や谷の底（最適解）をどうやって効率よく見つけるかという「探索戦略」の違いによって、いくつかの系統に分けられます。

大きく分けると、**「勾配（傾き）を使うもの」**と、**「数理的な構造を活かすもの」**、そして**「生物などの動きを模倣したもの」**があります。

---

## 1. 勾配ベースのアルゴリズム（主流派）

現在の機械学習やディープラーニングで最もよく使われるグループです。「今の場所の傾き」を見て、どちらに進むべきかを決めます。

* **勾配降下法 (Gradient Descent)**
* **特徴**: 最も基本。坂を一番急な方向へ下る。
* **弱点**: データの量が多いと計算が非常に重くなる。


* **確率的勾配降下法 (SGD)**
* **特徴**: データを1つずつ（または小分けにして）見て更新する。計算が速く、変な溝（局所解）から抜け出しやすい。


* **Adam (Adaptive Moment Estimation)**
* **特徴**: 「慣性（今まで進んできた勢い）」と「学習率の自動調整」を組み合わせた、現在最強クラスの汎用アルゴリズム。



## 2. 数理的な構造を活かすアルゴリズム（理論派）

関数の「カド」や「制約条件」が厳しい場合に、頭脳的なアプローチで解くグループです。

* **近接勾配法 (Proximal Gradient Method)**
* **特徴**: Lasso回帰のように微分できない「カド」がある場合に、「勾配ステップ」と「カドの調整（近接写像）」を交互に行う。


* **ADMM (交互方向乗数法)**
* **特徴**: 大きな問題を小さな問題に分割して解く。分散計算が得意で、複雑な制約条件があるときに非常に強力。


* **ニュートン法**
* **特徴**: 1次微分だけでなく、2次微分（曲がり具合）も使う。正解にたどり着くまでのステップ数が極めて少ないが、計算コストが高い。


数理的な構造（関数の凸性、対称性、制約の形など）を巧みに利用するアルゴリズムは、他にも非常に強力なものが存在します。これらは特に、 **「精度が極めて重要」** なシーンや **「リアルタイム性が求められる」** 制御などの分野で活躍します。

代表的なものをいくつか挙げます。


## 1. 内点法 (Interior Point Method)

不等式制約（ など）がある問題を解くための王道アルゴリズムです。

* **特徴**: 領域の外側から攻めるのではなく、**領域の内側を通りながら**最適解（多くの場合、境界線上にある）に向かいます。
* **仕組み**: 境界に近づくと無限大に発散する「バリア関数」を本来の目的に足し合わせることで、解が領域の外に出ないように強制します。
* **強み**: 線形計画法や二次計画法において、非常に大規模な問題でも安定して高速に解けることが理論的に証明されています。

---

## 2. 逐次二次計画法 (SQP: Sequential Quadratic Programming)

非線形な制約条件（曲がった境界線など）がある複雑な最適化問題に使われます。

* **特徴**: 難しい問題を、**「扱いやすい二次計画問題（プログラミングで一瞬で解ける形）」に細かく分割して解く**アプローチです。
* **仕組み**: 今いる地点で関数を二次関数として近似し、制約を直線として近似して、そのミニ問題を解く。これを繰り返します。
* **強み**: ロボットの経路計画や化学プラントの制御など、物理的な制約がガチガチにかかっている実世界の問題で多用されます。

---

## 3. 共役勾配法 (Conjugate Gradient Method)

巨大な連立一次方程式（）を解くことと、二次関数の最小化が同じであるという性質を利用した手法です。

* **特徴**: 最速降下法（普通の勾配降下法）が「同じ場所をジグザグに進んで無駄が多い」のに対し、**「一度探索した方向には二度と戻らない（共役な方向へ進む）」**という数学的工夫をしています。
* **強み**: メモリ消費が非常に少なく、数百万次元を超えるようなスパース行列（中身がスカスカの行列）の問題を解くのに最適です。

---

## 4. 座標降下法 (Coordinate Descent)

「一度に全部の変数を動かすのが難しいなら、一つずつ動かせばいい」という、シンプルながら強力な構造利用です。

* **特徴**:  以外を固定して  を最適化、次に  以外を固定して  を最適化……という手順を繰り返します。
* **強み**: **Lasso回帰の標準的な解法**として有名です（`scikit-learn`のLassoもこれを使っています）。各ステップの計算が非常に単純な数式になる構造を活かしており、高速です。

---

## 5. 信頼領域法 (Trust Region Method)

ニュートン法などの「一気にジャンプする」手法の危うさを克服した手法です。

* **特徴**: 「今の地点の周り、**この半径（信頼領域）の中だけなら**、自分の立てた近似式が正しいはずだ」という予測に基づき、その範囲内でのベストを探します。
* **強み**: 勾配がゼロに近い場所や、関数の形が急激に変わる場所でも、ステップサイズを自動調整して「大失敗」を防ぎながら着実に進みます。

---

### まとめ：使い分けのヒント

| アルゴリズム | 向いている状況 |
| --- | --- |
| **内点法** | 金融ポートフォリオ最適化など、厳しい不等式制約があるとき |
| **SQP** | ロボットアームの動きなど、物理法則（非線形制約）があるとき |
| **共役勾配法** | 物理シミュレーションなど、超巨大な行列を扱うとき |
| **座標降下法** | Lassoなど、各変数の最適化が個別に計算しやすいとき |

数理的構造を活かすアルゴリズムは、**「問題の形をよく観察して、それに特化した道具を選ぶ」**という職人技のような面白さがあります。


---

## 3. メタヒューリスティクス（生物・自然模倣派）

関数の形が全く分からなかったり、不連続だったりする場合に、数理的なルールではなく「運」や「集団の知恵」で探すグループです。

* **遺伝的アルゴリズム (GA)**
* **特徴**: 生物の進化（交配・突然変異・選択）を模倣する。設計の最適化など、答えの候補が膨大な時に強い。


* **粒子群最適化 (PSO)**
* **特徴**: 鳥の群れの動きを模倣する。各粒子が自分のベストと群れのベストを共有しながら動く。


* **焼きなまし法 (Simulated Annealing)**
* **特徴**: 金属の冷却過程を模倣。最初は大きくランダムに動き、徐々に動きを小さくして安定させる。



---

## どのアルゴリズムを選ぶべき？

エンジニアが実務で選ぶ際のシンプルな指針は以下の通りです。

1. **ディープラーニング・大規模な学習**
 **Adam** 一択でOK（迷ったらこれ）。
2. **変数を削りたい、スパースにしたい（Lassoなど）**
 **近接勾配法（FISTA）**。
3. **複雑な数理制約（ など）がある**
 **ADMM**。
4. **数式が書けない、微分もできない特殊な問題**
 **遺伝的アルゴリズム（GA）**。




