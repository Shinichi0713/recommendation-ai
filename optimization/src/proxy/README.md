**Proximal Gradient Method（近接勾配法）** は、最適化したい関数が「きれいな部分」と「カドのある部分」に分かれているときに使われる、非常にスマートなアルゴリズムです。

先日に解説したADMMとも深い関わりがありますが、よりシンプルに「勾配降下法の進化版」として捉えることができます。

## 概要

### 1. どんな問題を解くのか？

近接勾配法は、次のような **「和」** の形をした目的関数の最小化を得意とします。

$$f(x) + g(x)$$

* $f(x)$：滑らかな関数
（微分が可能。例：最小二乗法の誤差項 $\|Ax - b\|^2$）
* $g(x)$：滑らかではない関数
（カドがあって微分できない。例：Lasso回帰の $L_1$ ノルム $\|x\|_1$）

通常の勾配降下法は $g(x)$ のカド（微分不能点）で立ち往生してしまいますが、近接勾配法はこの問題を鮮やかに解決します。

### 2. アルゴリズムの仕組み

考え方は非常にシンプルです。 **「$f(x)$ については普通に坂を下り、$g(x)$ については専用の調整役（近接写像）に任せる」** という2ステップを踏みます。

#### ステップ1：勾配ステップ（Gradient Step）

まず、$g(x)$ を無視して $f(x)$ だけを見て、普通の勾配降下法を一歩進めます。

$$y^{k} = x^k - \alpha \nabla f(x^k)$$

（$\alpha$ は学習率です）

#### ステップ2：近接ステップ（Proximal Step）

一歩進んだ先（$y^k$）に対して、$g(x)$ の影響を考慮して微調整を行います。

$$x^{k+1} = \text{prox}_{\alpha g}(y^k)$$

この **（近接写像）** という演算が、カドのある関数 $g(x)$ に対して「坂を下る」代わりの役割を果たします。


### 3. なぜ「近接写像（Proximal Operator）」がすごいのか？

近接写像 $\text{prox}_{\alpha g}(y)$ は、数式で書くと「$y$ の近くにいて、かつ $g(x)$ を小さくする点を探す」という小さな最適化問題なのですが、**主要な関数については「答え」が公式として決まっています。**

* $g(x)$ が $L_1$ ノルムの場合：
答えは「ソフトしきい値演算」になります。
一定以下の小さな値を 0 にギュッと押しつぶす処理です。これにより、計算が爆速になります。

### 4. ADMMとの違いは？

よく似ている2つですが、使い分けのポイントは **「制約条件の複雑さ」** です。

| 特徴 | Proximal Gradient Method | ADMM |
| --- | --- | --- |
| **得意な形** | $f(x) + g(x)$ | $f(x) + g(x)$$f(x) + g(z)$ かつ $Ax+Bz=c$ |
| **強み** | アルゴリズムが極めてシンプルで高速 | 複雑な制約条件（行列 $A, B$）があっても解ける |
| **位置づけ** | 勾配降下法の拡張 | 拡張ラグランジュ法の拡張 |

* **エンジニアの判断**: 変数が直接 $L_1$ ノルムにかかっているだけなら Proximal Gradient、変数同士が数式で複雑に絡み合っている（制約がある）なら ADMM を選ぶのが定石です。


### 5. 代表的な派生：ISTA と FISTA

近接勾配法の名前を聞くと、セットで出てくるのがこれらです。

* **ISTA (Iterative Soft-Thresholding Algorithm)**：
まさに $L_1$ ノルムに近接勾配法を適用した基本形です。
* **FISTA (Fast ISTA)**：
ISTAに「加速（慣性）」のアイデアを取り入れたものです。
計算の手間はほとんど変わらないのに、**収束スピードが劇的に（理論上 $O(1/k^2)$ まで）アップ**します。画像処理や信号処理の現場で最もよく使われるアルゴリズムの一つです。


### まとめ

Proximal Gradient Methodは、**「微分できる部分は勾配で、微分できない部分は数式（近接写像）で」**という、適材適所のハイブリッドな解法です。

この「FISTA」の加速の仕組みや、実際にPythonで数行で実装するコードに興味はありますか？（実は、一歩前の値との差を少し足すだけで加速できるんです！）