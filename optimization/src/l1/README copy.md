Lasso回帰は、最適化の世界において **「滑らかな誤差」と「カドのある制約」を融合させた、スパース最適化（Sparse Optimization）の入り口** という非常に重要な立ち位置にあります。

エンジニアやデータサイエンティストにとって、Lassoは単なる「回帰の手法」ではなく、特定の数理構造を持つ最適化問題の代表例として扱われます。

本日は、Lasso回帰の前に最小２乗誤差の最小化についての説明を行い、その上でLasso回帰そのものについて説明を行います。

## 最小二乗法が微分可能
最小二乗法の誤差項  $\|Ax - b\|^2$ が微分可能（滑らか）である理由は、この式を分解してみると **「多項式（2次関数）」の組み合わせにすぎないから** です。

数学的な直感と、エンジニアがコードを書く際の視点の両方から解説します。

### 1. 数学的な構造：なぜ「カド」がないのか

$L_2$ ノルム（ユークリッド距離）の2乗である $\|Ax - b\|^2$ を展開すると、その正体がわかります。

$x$ を変数とする関数 $f(x)$ として展開すると：

$$f(x) = (Ax - b)^T (Ax - b)$$

$$f(x) = x^T A^T A x - 2b^T A x + b^T b$$

この式をよく見ると、以下の要素で構成されていることがわかります。

1. $x^T (A^T A) x$： $x$ の 2次項（放物線の多変数版）
2. $-2(A^T b)^T x$： $x$ の 1次項（直線）
3. $b^T b$： $x$ に関わらない 定数項

 $y = ax^2 + bx + c$ という放物線がどこでも滑らかに微分できるように、この式も $x$ の各成分について微分が可能です。

 **「2乗」という操作が、マイナスをプラスに反転させつつ、原点でカドを作らずになだらかに繋いでくれる**のです。


### 2. $L_1$ ノルム（絶対値）との決定的な違い

微分できない代表例である $L_1$ ノルム $\|x\|_1 = \sum |x_i|$ と比較すると、理由がより鮮明になります。

| 特徴 | $L_2$ ノルムの2乗 ($\|x\|^2 = x^2$) | $L_1$ ノルム ($\|x\| = |x|$) |
| :--- | :--- | :--- |
| **グラフの形状** | 放物線（U字型） | V字型 |
| **原点 (0) での挙動** | **なだらか**。傾きが 0 に近づく。 | **急激**。折れ曲がっている。 |
| **微分 ($f'(x)$)** | $2x$（連続） | $1$ または $-1$ （不連続） |

$L_1$ ノルムは原点で「カド」があるため、そこで右から近づいたときの傾きと左から近づいたときの傾きが一致せず、微分が定義できません。
一方、最小二乗法の誤差項（$L_2$）は、2乗のおかげで原点付近でも非常に滑らかです。

### 3. 微分した結果（勾配）はどうなるか？

実際に微分（勾配 $\nabla f(x)$ を計算）すると、非常に綺麗な形になります。

$$\nabla f(x) = 2 A^T (Ax - b)$$

$$\nabla f(x) = 2 A^T (Ax - b)$$この結果も $x$ に関する線形（1次）の式であり、どこにも「不連続な点」や「定義できない場所」がありません。
エンジニアが勾配降下法を実装する際、`grad = 2 * A.T @ (A @ x - b)` と一行で書けるのは、この関数が全領域で微分可能だからです。


### 4. エンジニアにとっての「微分可能」のメリット

1. **勾配降下法がそのまま使える**:
「今の場所の傾き」が常に計算できるため、少しずつ坂を下るだけで最適解にたどり着けます。
2. **解析的に解ける（正規方程式）**:
微分した結果を「$= 0$」とおくことで、計算機で反復計算をしなくても $x = (A^T A)^{-1} A^T b$ という一発の行列計算で答えを出すことも可能です。

## Lasso回帰について

Lasso回帰（Least Absolute Shrinkage and Selection Operator）は、一言で言えば **「不要な変数を自動的に削ってくれる、賢い線形回帰」** です。

通常の最小二乗法に「重みの絶対値（ノルム）」というペナルティを加えることで、重要度の低いデータの係数を**ピッタリ0**にしてくれます。


### 1. Lasso回帰の数式

Lassoの目的関数は、先ほど登場した「滑らかな部分」と「カドのある部分」の足し算で構成されています。

$$\min_{x} \underbrace{\|Ax - b\|^2}_{\text{最小二乗誤差}} + \underbrace{\lambda \|x\|_1}_{\text{L1正則化項}}$$

* **第1項（最小二乗誤差）**: データの予測精度を上げようとする（$L_2$の性質：微分可能）。
* **第2項（L1正則化項）**: 重み  の合計を小さく抑えようとする（$L_1$の性質：原点でカドがある）。
* **$\lambda$(ラムダ)**: どれくらい厳しく変数を削るかを調整するパラメータです。


### 2. なぜ変数が「0」になるのか？（カドの魔法）

これがLassoの最大の特徴です。なぜ「小さくなる」だけでなく「0」になるのでしょうか？

視覚的に理解すると、最小二乗法の誤差範囲（楕円形）が、$L_1$ノルムの制約範囲（ひし形）と最初に接触する場所を探すことになります。

* **ひし形には「角（カド）」がある**: ひし形の角は、軸の上（つまり、ある変数が0の地点）にあります。
* **角で接触しやすい**: 楕円が膨らんでいったとき、丸い円よりも「尖った角」に当たりやすいため、結果として多くの係数が0に選ばれます。


### 3. Lassoを使う3つのメリット

#### ① 特徴量選択 (Feature Selection)

何百ものデータ項目（例：血液検査の100項目）があっても、病気に本当に関係のある3項目だけを残し、残りを0にしてくれます。エンジニアが手動で変数を選ぶ手間を省けます。

#### ② モデルの解釈性が高まる

「この予測には、変数AとBだけが効いています」と断言できるため、ブラックボックス化しにくい（人間に説明しやすい）モデルになります。

#### ③ 過学習（Overfitting）を防ぐ

余計なノイズに反応しないよう重みを制限するため、未知のデータに対しても安定した予測ができるようになります。


### 4. アルゴリズムの繋がり

ここで、これまでの話が一本の線に繋がります。

1. **問題**: Lassoは $L_1$ ノルムを含んでいるので、**原点で微分できない！**
2. **解決法1**: **Proximal Gradient Method (ISTA/FISTA)** を使う。
* 誤差項（$L_2$）を勾配で下り、正則化項（$L_1$）をソフトしきい値演算で「0」に飛ばす。


3. **解決法2**: 変数がさらに複雑に絡むなら **ADMM** を使う。

### 5. 実装イメージ (scikit-learn)

Pythonの `scikit-learn` を使えば、エンジニアはアルゴリズムの裏側を意識せずとも数行で実行できます。

```python
from sklearn.linear_model import Lasso

# alphaが数式のlambdaに相当
model = Lasso(alpha=0.1)
model.fit(X_train, y_train)

# 0になった係数を確認
print(model.coef_) 
```

### まとめ：Lassoとは

* **正体**: 最小二乗法 ＋ $L_1$ 正則化。
* **強み**: **変数をゼロにする**（スパース性）。
* **使い時**: データ項目が多すぎて、どれが重要か絞り込みたい時。

次は、Lassoとよく比較される **「Ridge（リッジ）回帰」** との違いについて、図解を交えて解説しましょうか？（Ridgeは変数を0にはせず、全体的に小さくするタイプです）
