最適化や機械学習における**正則化（Regularization）**とは、一言で言えば **「モデルにわざと『制限』を課すことで、未知のデータに対する予測性能（汎化性能）を高める技術」** のことです。

たとえるなら、**「練習問題の答えを丸暗記しようとする学生に、あえて難しい制約を与えて、応用力を身につけさせる」**ような仕組みです。


## 1. なぜ正則化が必要なのか？（過学習の防止）

モデルが複雑すぎると、手元にあるデータ（訓練データ）の細かいノイズまで完璧に再現しようとしてしまいます。これを **「過学習（オーバーフィッティング）」** と呼びます。

* **正則化なし**: 訓練データの全ての点を通るように、激しく蛇行した複雑な線を引く。
* **正則化あり**: 多少のズレは許容しつつ、全体として滑らかでシンプルな線を引く。


## 2. 正則化の数学的な仕組み

正則化では、本来の目的である「誤差（損失関数）の最小化」に、 **「重み（パラメータ）の大きさに対するペナルティ」** をプラスします。

アルゴリズムは「誤差を減らしたい」と思う一方で「重みを大きくすると怒られる（コストが上がる）」という板挟みになります。その結果、 **「本当に重要なデータだけに反応し、余計な数値は小さく抑える」** という絶妙なバランスが生まれます。

---

## 3. 代表的な2つの正則化

これまでの議論に登場した「リッジ」や「ラッソ」がまさにこれに当たります。

### ① L2正則化（リッジ）

* **やり方**: 重みの「2乗」をペナルティにする。
* **効果**: 重みを全体的にまんべんなく小さくする。
* **立ち位置**: 計算が安定し、急激な予測の変化を抑える「ブレーキ」の役割。

### ② L1正則化（ラッソ）

* **やり方**: 重みの「絶対値」をペナルティにする。
* **効果**: 不要な変数の重みを**ピッタリ 0** にする。
* **立ち位置**: 重要な情報だけを抜き出す「フィルター」の役割。

---

## 4. 日常生活で例えると？

正則化は、**「説明のシンプルさ」**を求める感覚に似ています。

* **過学習な説明**: 「昨日の株価が上がったのは、気温が25.3度で、近所の猫が3回鳴いて、隣の家の山田さんが赤いネクタイをしていたからです」
* **正則化された説明**: 「昨日の株価が上がったのは、景気指数が改善したからです」

後者の方が、他の日（未知のデータ）にも通用する「応用力」があることがわかります。正則化は、数式を通じてこの「本質だけを見る力」をモデルに強制的に持たせる技術なのです。

---

### まとめ

正則化とは、**「複雑すぎるモデルを適度に『怠けさせる』ことで、本質的なルールを見つけやすくする調整作業」**と言えます。

次は、実際に深層学習などでよく使われる、**「重み減衰（Weight Decay）」とL2正則化がなぜ同じ意味になるのか**、その繋がりを解説しましょうか？