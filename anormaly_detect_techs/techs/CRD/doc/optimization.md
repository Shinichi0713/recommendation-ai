CRD（Collaborative Representation-based Detector）の最適化は、数学的には **「正則化付き最小二乗問題」** として定義されます。この最適化の美しさは、反復計算（ループ）を必要とせず、 **行列演算一発で「解析解（Closed-form solution）」が導き出せる** 点にあります。

理論的な背景と、具体的な最適化の手順を順を追って説明します。

---

### 1. 最適化の目的関数

CRDが最小化しようとしている数式（目的関数）は、以下の通りです。

$$
J(\alpha) = \|y - D\alpha\|_2^2 + \lambda \|\alpha\|_2^2
$$

ここで、各変数の役割は以下の通りです：

* $y$: ターゲット画素のベクトル
* $D$: 背景画素を並べた辞書行列
* $\alpha$: 各背景画素がターゲットを再現するために負担する「重み」のベクトル
* $\lambda$: 正則化パラメータ。重み $\alpha$ が極端に大きくなるのを防ぎ、「みんなで協力（Collaborative）」させるための調整役です。


### 2. 理論：なぜ「協調」が生まれるのか（$L_2$ 正則化）

この式で最も重要なのは第2項の $\|\alpha\|_2^2$ です。これは数学的に Ridge（リッジ）正則化 と呼ばれます。

* **スパース（$L_1$）との違い**: スパース表現（SRD）では一部の画素に重みを集中させますが、CRDの $L_2$ 正則化は重みを「広く浅く」分散させます。
* **効果**: これにより、辞書内の特定の画素にノイズがあっても、他の画素がそれを補うため、再構成が非常に安定します。

---

### 3. 最適化の手順：最小値の導出

この目的関数 $J(\alpha)$ は、$\alpha$ に関して下に凸な二次関数であるため、微分して $0$ と置くことで最小値を求められます。

1. **微分する**:

$$
\frac{\partial J}{\partial \alpha} = -2D^T(y - D\alpha) + 2\lambda\alpha = 0
$$

2. **式を整理する**:

$$(D^T D + \lambda I)\alpha = D^T y$$

3. **解（$\alpha$）を導き出す**:

$$\alpha = (D^T D + \lambda I)^{-1} D^T y$$

この $\alpha$ こそが、ターゲットを最も効率的かつ安定的に再現するための「魔法の配合比率」です。

---

### 4. プロジェクション行列（投影行列）による高速化

CRDの最適化における最大の知恵は、**「ターゲット $y$ が何であっても、辞書 $D$ さえ決まれば計算の半分は終わる」**という点です。

上記の式を以下のように書き換えます。

$$\alpha = P y \quad \text{where} \quad P = (D^T D + \lambda I)^{-1} D^T$$

この $P$ を**プロジェクション行列（投影行列）**と呼びます。一度 $P$ を計算してしまえば、あとはターゲット  $y$  と行列の掛け算をするだけで最適化が完了します。

---

### 5. 異常スコアへの変換

最適化された $\alpha$ を使ってターゲットを再構成し、元の $y$ との「ズレ」を計算します。

$$\text{Score} = \|y - D\alpha\|_2^2$$

* **正常な場合**: $\alpha$ によって $y$ が完璧に再現されるため、スコアは $0$ に近くなります。
* **異常な場合**: 辞書 $D$ にない成分（色や波長）を持っているため、どんなに $\alpha$ を最適化しても再現しきれず、大きなスコアが残ります。

---

### 6. まとめ：CRD最適化の3大特徴

* **解析解**: 反復計算（ISTAやFISTAなど）が不要。
* **安定性**: 正則化項  により、行列が特異（背景が単調）でも逆行列計算が破綻しにくい。
* **決定性**: 同じデータに対しては常に同じ最適解が瞬時に得られる。

### 次のステップへの提案

この理論を理解すると、**「 の値をどう決めるか」**が次の重要なポイントになります。 を大きくしすぎると背景の再現が甘くなり、小さすぎるとノイズに敏感になります。

この  の調整方法や、より計算を安定させるための**「正規化（Normalization）」**のテクニックについて、さらに詳しく解説しましょうか？


## リッジ正則化

CRD（Collaborative Representation-based Detector）を実際に解くために使われているアルゴリズムは、数学的には**「リッジ回帰（Ridge Regression）」**、または信号処理の文脈では**「正則化付き最小二乗法（Regularized Least Squares: RLS）」**と呼ばれます。

この手法は、反復計算（ループによる近似）を必要とせず、行列演算だけで「一発で」最適解に到達できる**解析解（Closed-form solution）**を持つのが最大の特徴です。

具体的な手順を、実装のフローに沿って解説します。

---

### CRDを解くための具体的ステップ

ターゲット画素  に対して異常スコアを算出するまでの手順は、以下の**5つのステップ**で構成されます。

#### 1. データの正規化（Preprocessing）

数値計算を安定させるため、ターゲット画素  および背景辞書  の各ベクトル（列）を正規化します。

* 各ベクトルをそのL2ノルム（長さ）で割り、単位ベクトルにします。これにより、明るさの変化の影響を抑え、純粋な「成分の構成比（色やスペクトルの形）」で比較できるようになります。

#### 2. 相関行列の計算

背景辞書 （サイズ：）から、背景画素同士の類似度を示す行列を作ります。

* 計算式： 
* これにより  （背景画素数 × 背景画素数）の行列が得られます。

#### 3. 正則化項の追加

計算の安定化と「協調性」を担保するために、対角要素に微小な値  を足します。

* 計算式： 
*  は単位行列です。これにより、背景に似たような画素が多く含まれていても（多重共線性）、逆行列が計算可能になります。

#### 4. 重み係数  の算出（正規方程式の解法）

ターゲット  を再現するために、どの背景画素をどれだけ使うかの比率  を求めます。

* **解法**: 
* 実際の実装（Pythonの `scipy.linalg.solve` など）では、逆行列を直接計算するよりも、**コレスキー分解（Cholesky decomposition）**などの高速な連立一次方程式の解法が内部で使われます。

#### 5. 再構成誤差（異常スコア）の算出

得られた最適な重み  を使って、背景画素でターゲットを「再現」してみます。

* 再構成ベクトル： 
* 異常スコア： 
* この「どうしても再現できなかった差分」が、背景には存在しない**異常成分**として出力されます。

---

### アルゴリズムとしての呼び名

この一連の手順は、以下の名前で呼ばれることもあります。

* **正規方程式 (Normal Equation)**: 最小二乗問題を解くための標準的な行列方程式。
* **ティコノフ正則化 (Tikhonov Regularization)**: 不適切な問題（特異行列に近い場合）を解くための数学的枠組み。
* **クローズドフォーム・ソリューション**: ループを回さずに数式一発で答えが出る解法。

### まとめ：なぜこの手順が選ばれるのか

CRDがこの「リッジ回帰」の手順を踏む理由は、**「計算の効率」**と**「ノイズへの強さ」**の両立です。

* **効率**: 画像全体をスキャンする場合、一画素ごとに反復計算（SRDなど）をしていたら時間がかかりすぎますが、CRDはこの行列演算のおかげで実用的な速度で動作します。
* **強さ**: 周囲の画素「全員」に役割を振るため、辞書に多少のゴミ（ノイズ）が混じっていても、結果が大きくブレません。




