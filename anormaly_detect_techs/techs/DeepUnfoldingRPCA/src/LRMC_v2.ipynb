{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2-azWb8dber",
        "outputId": "f077a19b-7ebd-49a4-dfe0-990b1256129b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NEU_Seg'...\n",
            "remote: Enumerating objects: 8947, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 8947 (delta 0), reused 0 (delta 0), pack-reused 8945 (from 1)\u001b[K\n",
            "Receiving objects: 100% (8947/8947), 35.08 MiB | 11.53 MiB/s, done.\n",
            "Dataset successfully downloaded.\n",
            "Total images found: 0\n",
            "Example filenames: []\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# 1. リポジトリをクローン\n",
        "!git clone https://github.com/DHW-Master/NEU_Seg.git\n",
        "\n",
        "# 2. データのパスを確認\n",
        "# NEU_Seg内には 'images' フォルダに画像が含まれています\n",
        "dataset_root = \"NEU_Seg/images\"\n",
        "\n",
        "if os.path.exists(dataset_root):\n",
        "    print(\"Dataset successfully downloaded.\")\n",
        "    # フォルダ内のファイル数を表示\n",
        "    files = [f for f in os.listdir(dataset_root) if f.endswith('.jpg')]\n",
        "    print(f\"Total images found: {len(files)}\")\n",
        "    # ファイル名の例を表示（Cr, In, Pa, PS, Sc, RSなどの略称が含まれます）\n",
        "    # 例: Cr_1.jpg (Cracks: ひび割れ)\n",
        "    print(\"Example filenames:\", files[:5])\n",
        "else:\n",
        "    print(\"Failed to download the dataset.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "├──./datasets/\n",
        "│    ├── Dataset_name\n",
        "│    │    ├── train\n",
        "│    │    │    ├── images\n",
        "│    │    │    │    ├── 000001.png\n",
        "│    │    │    │    ├── ...\n",
        "│    │    │    ├── masks\n",
        "│    │    │    │    ├── 000001.png\n",
        "│    │    │    │    ├── ...\n",
        "│    │    ├── test\n",
        "│    │    │    ├── images\n",
        "│    │    │    │    ├── 000002.png\n",
        "```"
      ],
      "metadata": {
        "id": "BqHUtdFod6kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/datasets"
      ],
      "metadata": {
        "id": "Wz3EypuztTgT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from glob import glob\n",
        "\n",
        "# 1. 設定：新しいディレクトリ構造の定義\n",
        "base_dir = \"./datasets/NEU_Seg_Custom\"\n",
        "structure = [\n",
        "    \"train/images\", \"train/masks\",\n",
        "    \"test/images\", \"test/masks\"\n",
        "]\n",
        "\n",
        "for path in structure:\n",
        "    os.makedirs(os.path.join(base_dir, path), exist_ok=True)\n",
        "\n",
        "# 2. パス設定\n",
        "src_img_dir = \"/content/NEU_Seg/images\"\n",
        "src_ann_dir = \"/content/NEU_Seg/annotations\"\n",
        "\n",
        "def organize_neu_data():\n",
        "    dirs_source = [\"test\", \"training\"] # 元リポジトリのサブフォルダ名\n",
        "\n",
        "    for dir_source in dirs_source:\n",
        "        # ソースディレクトリ内の全画像を取得\n",
        "        current_src_img_path = os.path.join(src_img_dir, dir_source)\n",
        "        all_images = sorted(glob(os.path.join(current_src_img_path, \"*.jpg\")))\n",
        "\n",
        "        print(f\"\\nScanning directory: {dir_source}\")\n",
        "\n",
        "        # マスクが存在する有効なペアのみを抽出\n",
        "        valid_pairs = []\n",
        "        for img_path in all_images:\n",
        "            fname = os.path.basename(img_path)\n",
        "            mask_name = fname.replace(\".jpg\", \".png\")\n",
        "\n",
        "            # マスクの存在確認（複数の候補地をチェック）\n",
        "            potential_mask_paths = [\n",
        "                os.path.join(src_ann_dir, mask_name),\n",
        "                os.path.join(src_ann_dir, \"test\", mask_name),\n",
        "                os.path.join(src_ann_dir, \"train\", mask_name),\n",
        "                os.path.join(src_ann_dir, \"training\", mask_name)\n",
        "            ]\n",
        "\n",
        "            found_mask_path = None\n",
        "            for m_path in potential_mask_paths:\n",
        "                if os.path.exists(m_path):\n",
        "                    found_mask_path = m_path\n",
        "                    break\n",
        "\n",
        "            # マスクが見つかった場合のみリストに追加\n",
        "            if found_mask_path:\n",
        "                valid_pairs.append((img_path, found_mask_path))\n",
        "\n",
        "        print(f\"Total images found: {len(all_images)}\")\n",
        "        print(f\"Valid pairs with masks: {len(valid_pairs)}\")\n",
        "\n",
        "        if len(valid_pairs) == 0:\n",
        "            continue\n",
        "\n",
        "        # 8:2 で分割\n",
        "        split_idx = int(len(valid_pairs) * 0.8)\n",
        "        train_pairs = valid_pairs[:split_idx]\n",
        "        test_pairs = valid_pairs[split_idx:]\n",
        "\n",
        "        def copy_valid_files(pairs, target_sub_dir):\n",
        "            for img_src, mask_src in pairs:\n",
        "                fname = os.path.basename(img_src)\n",
        "                mname = os.path.basename(mask_src)\n",
        "\n",
        "                # 画像のコピー\n",
        "                shutil.copy(img_src, os.path.join(base_dir, target_sub_dir, \"images\", fname))\n",
        "                # マスクのコピー\n",
        "                shutil.copy(mask_src, os.path.join(base_dir, target_sub_dir, \"masks\", mname))\n",
        "\n",
        "        print(f\"Copying {len(train_pairs)} pairs to Train...\")\n",
        "        copy_valid_files(train_pairs, \"train\")\n",
        "\n",
        "        print(f\"Copying {len(test_pairs)} pairs to Test...\")\n",
        "        copy_valid_files(test_pairs, \"test\")\n",
        "\n",
        "    print(\"\\nData organization complete!\")\n",
        "\n",
        "organize_neu_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VizoGhmZtFXU",
        "outputId": "e2194a95-2b00-4b00-f4ff-994c32311c4d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scanning directory: test\n",
            "Total images found: 840\n",
            "Valid pairs with masks: 840\n",
            "Copying 672 pairs to Train...\n",
            "Copying 168 pairs to Test...\n",
            "\n",
            "Scanning directory: training\n",
            "Total images found: 3630\n",
            "Valid pairs with masks: 3630\n",
            "Copying 2904 pairs to Train...\n",
            "Copying 726 pairs to Test...\n",
            "\n",
            "Data organization complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as Data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Datasetの定義\n",
        "class NEUSegDataset(Dataset):\n",
        "    def __init__(self, img_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.img_names = [f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.png'))]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_names[idx])\n",
        "        image = Image.open(img_path).convert(\"L\") # グレースケール\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "# 2. RPCANet モデルの実装 (Deep Unfolding RPCA)\n",
        "class RPCANet(nn.Module):\n",
        "    def __init__(self, layers=5):\n",
        "        super(RPCANet, self).__init__()\n",
        "        self.layers = layers\n",
        "        # ISTAのステップサイズとしきい値を層ごとに学習\n",
        "        self.eta = nn.Parameter(torch.ones(layers) * 0.1)\n",
        "        self.theta = nn.Parameter(torch.ones(layers) * 0.01)\n",
        "\n",
        "        # 変換行列（畳み込み層として実装することで近傍情報を活用）\n",
        "        self.conv_W = nn.ModuleList([\n",
        "            nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=False) for _ in range(layers)\n",
        "        ])\n",
        "        for conv in self.conv_W:\n",
        "            nn.init.constant_(conv.weight, 1.0/9.0) # 平均化フィルタに近い初期値\n",
        "\n",
        "    def soft_threshold(self, x, theta):\n",
        "        return torch.sign(x) * torch.relu(torch.abs(x) - F.softplus(theta))\n",
        "\n",
        "    def forward(self, M):\n",
        "        S = torch.zeros_like(M)\n",
        "        for i in range(self.layers):\n",
        "            # L = M - S (低ランク成分の推定)\n",
        "            # 勾配降下ステップの展開\n",
        "            residual = M - S\n",
        "            grad = self.conv_W[i](residual)\n",
        "            S = self.soft_threshold(S + self.eta[i] * grad, self.theta[i])\n",
        "\n",
        "        L = M - S\n",
        "        return L, S\n",
        "\n",
        "class NEUSegDataset(Data.Dataset):\n",
        "    def __init__(self, base_dir, mode='train', base_size=256):\n",
        "        self.img_dir = os.path.join(base_dir, mode, 'images')\n",
        "        self.mask_dir = os.path.join(base_dir, mode, 'masks')\n",
        "        self.img_names = sorted([f for f in os.listdir(self.img_dir) if f.endswith('.jpg')])\n",
        "        self.base_size = base_size\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((base_size, base_size)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.img_names[idx]\n",
        "        img_path = os.path.join(self.img_dir, name)\n",
        "        mask_path = os.path.join(self.mask_dir, name.replace('.jpg', '.png'))\n",
        "\n",
        "        image = Image.open(img_path).convert('L')\n",
        "        mask = Image.open(mask_path).convert('L') # マスクもグレースケールで読み込み\n",
        "\n",
        "        image = self.transform(image)\n",
        "        mask = self.transform(mask)\n",
        "\n",
        "        # マスクを0or1のバイナリにする（SoftIoU用）\n",
        "        mask = (mask > 0.5).float()\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nrWVykBdfbom"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Avg_ChannelAttention_n(nn.Module):\n",
        "    def __init__(self, channels, r=4):\n",
        "        super(Avg_ChannelAttention_n, self).__init__()\n",
        "        self.avg_channel = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),  # 全局平均池化 bz,C_out,h,w -> bz,C_out,1,1\n",
        "            nn.Conv2d(channels, channels // r, 1, 1, 0),  # bz,C_out,1,1 -> bz,C_out/r,1,1\n",
        "            nn.BatchNorm2d(channels // r),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(channels // r, channels, 1, 1, 0),  # bz,C_out/r,1,1 -> bz,C_out,1,1\n",
        "            nn.BatchNorm2d(channels),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.avg_channel(x)\n",
        "\n",
        "\n",
        "\n",
        "class Avg_ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, r=4):\n",
        "        super(Avg_ChannelAttention, self).__init__()\n",
        "        self.avg_channel = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),  # 全局平均池化 bz,C_out,h,w -> bz,C_out,1,1\n",
        "            nn.Conv2d(channels, channels // r, 1, 1, 0),  # bz,C_out,1,1 -> bz,C_out/r,1,1\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(channels // r, channels, 1, 1, 0),  # bz,C_out/r,1,1 -> bz,C_out,1,1\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.avg_channel(x)\n",
        "\n",
        "\n",
        "class AttnContrastLayer(nn.Module):\n",
        "    def __init__(self, channels, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=False):\n",
        "        super(AttnContrastLayer, self).__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(channels, channels, kernel_size=kernel_size, stride=stride,\n",
        "                              padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "\n",
        "        self.attn = Avg_ChannelAttention(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out_normal = self.conv(x)\n",
        "\n",
        "        theta = self.attn(x)\n",
        "\n",
        "        kernel_w1 = self.conv.weight.sum(2).sum(2)\n",
        "\n",
        "        kernel_w2 = kernel_w1[:, :, None, None]\n",
        "\n",
        "        out_center = F.conv2d(input=x, weight=kernel_w2, bias=self.conv.bias, stride=self.conv.stride,\n",
        "                              padding=0, groups=self.conv.groups)\n",
        "\n",
        "        return theta * out_center - out_normal\n",
        "\n",
        "\n",
        "\n",
        "class AttnContrastLayer_n(nn.Module):\n",
        "    def __init__(self, channels, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=False):\n",
        "        super(AttnContrastLayer_n, self).__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(channels, channels, kernel_size=kernel_size, stride=stride,\n",
        "                              padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "\n",
        "        self.attn = Avg_ChannelAttention_n(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_normal = self.conv(x)\n",
        "        theta = self.attn(x)\n",
        "\n",
        "\n",
        "        kernel_w1 = self.conv.weight.sum(2).sum(2)\n",
        "        kernel_w2 = kernel_w1[:, :, None, None]\n",
        "\n",
        "        out_center = F.conv2d(input=x, weight=kernel_w2, bias=self.conv.bias, stride=self.conv.stride,\n",
        "                              padding=0, groups=self.conv.groups)\n",
        "\n",
        "        return theta * out_center - out_normal\n",
        "\n",
        "class AttnContrastLayer_d(nn.Module):\n",
        "    def __init__(self, channels, kernel_size=3, stride=1, padding=1, dilation=2, groups=1, bias=False):\n",
        "        super(AttnContrastLayer_d, self).__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(channels, channels, kernel_size=kernel_size, stride=stride,\n",
        "                              padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "\n",
        "        self.attn = Avg_ChannelAttention(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out_normal = self.conv(x)\n",
        "\n",
        "        theta = self.attn(x)\n",
        "\n",
        "        kernel_w1 = self.conv.weight.sum(2).sum(2)\n",
        "        kernel_w2 = kernel_w1[:, :, None, None]\n",
        "        out_center = F.conv2d(input=x, weight=kernel_w2, bias=self.conv.bias, stride=self.conv.stride,\n",
        "                              padding=0, groups=self.conv.groups)\n",
        "\n",
        "        return out_center - theta * out_normal\n",
        "\n",
        "class AtrousAttnWeight(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(AtrousAttnWeight, self).__init__()\n",
        "        self.attn = Avg_ChannelAttention(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.attn(x)"
      ],
      "metadata": {
        "id": "bNGujJTLsFos"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops import rearrange, repeat\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "__all__ = ['RPCANet9','RPCANet_LSTM']\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, bias=True, res_scale=1):\n",
        "\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.res_scale = res_scale\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=(kernel_size//2), bias=bias)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding=(kernel_size//2), bias=bias)\n",
        "        self.act1 = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.conv2(x)\n",
        "        res = x\n",
        "        x = res + input\n",
        "        return x\n",
        "\n",
        "class RPCANet9(nn.Module):\n",
        "    def __init__(self, stage_num=6, slayers=6, llayers=3, mlayers=3, channel=32, mode='train'):\n",
        "        super(RPCANet9, self).__init__()\n",
        "        self.stage_num = stage_num\n",
        "        self.decos = nn.ModuleList()\n",
        "        self.mode = mode\n",
        "        for _ in range(stage_num):\n",
        "            self.decos.append(DecompositionModule9(slayers=slayers, llayers=llayers,\n",
        "                                                  mlayers=mlayers, channel=channel))\n",
        "        for m in self.modules():\n",
        "            # 也可以判断是否为conv2d，使用相应的初始化方式\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                #nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, D):\n",
        "        T = torch.zeros(D.shape).to(D.device)\n",
        "        for i in range(self.stage_num):\n",
        "            D, T = self.decos[i](D, T)\n",
        "        if self.mode == 'train':\n",
        "            return D,T\n",
        "        else:\n",
        "            return T\n",
        "\n",
        "class DecompositionModule9(nn.Module):\n",
        "    def __init__(self, slayers=6, llayers=3, mlayers=3, channel=32):\n",
        "        super(DecompositionModule9, self).__init__()\n",
        "        self.lowrank = LowrankModule9(channel=channel, layers=llayers)\n",
        "        self.sparse = SparseModule9(channel=channel, layers=slayers)\n",
        "        self.merge = MergeModule9(channel=channel, layers=mlayers)\n",
        "\n",
        "    def forward(self, D, T):\n",
        "        B = self.lowrank(D, T)\n",
        "        T = self.sparse(D, B, T)\n",
        "        D = self.merge(B, T)\n",
        "        return D, T\n",
        "\n",
        "class LowrankModule9(nn.Module):\n",
        "    def __init__(self, channel=32, layers=3):\n",
        "        super(LowrankModule9, self).__init__()\n",
        "\n",
        "        convs = [nn.Conv2d(1, channel, kernel_size=3, padding=1, stride=1),\n",
        "                 nn.BatchNorm2d(channel),\n",
        "                 nn.ReLU(True)]\n",
        "        for i in range(layers):\n",
        "            convs.append(nn.Conv2d(channel, channel, kernel_size=3, padding=1, stride=1))\n",
        "            convs.append(nn.BatchNorm2d(channel))\n",
        "            convs.append(nn.ReLU(True))\n",
        "        convs.append(nn.Conv2d(channel, 1, kernel_size=3, padding=1, stride=1))\n",
        "        self.convs = nn.Sequential(*convs)\n",
        "\n",
        "    def forward(self, D, T):\n",
        "        x = D - T\n",
        "        B = x + self.convs(x)\n",
        "        return B\n",
        "\n",
        "class SparseModule9(nn.Module):\n",
        "    def __init__(self, channel=32, layers=6) -> object:\n",
        "        super(SparseModule9, self).__init__()\n",
        "        convs = [nn.Conv2d(1, channel, kernel_size=3, padding=1, stride=1),\n",
        "                 nn.ReLU(True)]\n",
        "        for i in range(layers):\n",
        "            convs.append(nn.Conv2d(channel, channel, kernel_size=3, padding=1, stride=1))\n",
        "            convs.append(nn.ReLU(True))\n",
        "        convs.append(nn.Conv2d(channel, 1, kernel_size=3, padding=1, stride=1))\n",
        "        self.convs = nn.Sequential(*convs)\n",
        "        self.epsilon = nn.Parameter(torch.Tensor([0.01]), requires_grad=True)\n",
        "\n",
        "    def forward(self, D, B, T):\n",
        "        x = T + D - B\n",
        "        T = x - self.epsilon * self.convs(x)\n",
        "        return T\n",
        "\n",
        "class MergeModule9(nn.Module):\n",
        "    def __init__(self, channel=32, layers=3):\n",
        "        super(MergeModule9, self).__init__()\n",
        "        convs = [nn.Conv2d(1, channel, kernel_size=3, padding=1, stride=1),\n",
        "                 nn.BatchNorm2d(channel),\n",
        "                 nn.ReLU(True)]\n",
        "        for i in range(layers):\n",
        "            convs.append(nn.Conv2d(channel, channel, kernel_size=3, padding=1, stride=1))\n",
        "            convs.append(nn.BatchNorm2d(channel))\n",
        "            convs.append(nn.ReLU(True))\n",
        "        convs.append(nn.Conv2d(channel, 1, kernel_size=3, padding=1, stride=1))\n",
        "        self.mapping = nn.Sequential(*convs)\n",
        "\n",
        "    def forward(self, B, T):\n",
        "        x = B + T\n",
        "        D = self.mapping(x)\n",
        "        return D\n",
        "\n",
        "\n",
        "class ConvLSTM(nn.Module):\n",
        "    def __init__(self, inp_dim, oup_dim, kernel):\n",
        "        super().__init__()\n",
        "        pad_x = 1\n",
        "        self.conv_xf = nn.Conv2d(inp_dim, oup_dim, kernel, padding=pad_x)\n",
        "        self.conv_xi = nn.Conv2d(inp_dim, oup_dim, kernel, padding=pad_x)\n",
        "        self.conv_xo = nn.Conv2d(inp_dim, oup_dim, kernel, padding=pad_x)\n",
        "        self.conv_xj = nn.Conv2d(inp_dim, oup_dim, kernel, padding=pad_x)\n",
        "\n",
        "        pad_h = 1\n",
        "        self.conv_hf = nn.Conv2d(oup_dim, oup_dim, kernel, padding=pad_h)\n",
        "        self.conv_hi = nn.Conv2d(oup_dim, oup_dim, kernel, padding=pad_h)\n",
        "        self.conv_ho = nn.Conv2d(oup_dim, oup_dim, kernel, padding=pad_h)\n",
        "        self.conv_hj = nn.Conv2d(oup_dim, oup_dim, kernel, padding=pad_h)\n",
        "\n",
        "    def forward(self, x, h, c):\n",
        "\n",
        "        if h is None and c is None:\n",
        "            i = F.sigmoid(self.conv_xi(x))\n",
        "            o = F.sigmoid(self.conv_xo(x))\n",
        "            j = F.tanh(self.conv_xj(x))\n",
        "            c = i * j\n",
        "            h = o * c\n",
        "        else:\n",
        "            f = F.sigmoid(self.conv_xf(x) + self.conv_hf(h))\n",
        "            i = F.sigmoid(self.conv_xi(x) + self.conv_hi(h))\n",
        "            o = F.sigmoid(self.conv_xo(x) + self.conv_ho(h))\n",
        "            j = F.tanh(self.conv_xj(x) + self.conv_hj(h))\n",
        "            c = f * c + i * j\n",
        "            h = o * F.tanh(c)\n",
        "\n",
        "        return h, h, c\n",
        "\n",
        "class RPCANet_LSTM(nn.Module):\n",
        "    def __init__(self, stage_num=6, slayers=6, mlayers=3, channel=32, mode='train'):\n",
        "        super(RPCANet_LSTM, self).__init__()\n",
        "        self.stage_num = stage_num\n",
        "        self.decos = nn.ModuleList()\n",
        "        self.mode = mode\n",
        "        for i in range(stage_num):\n",
        "            self.decos.append(DecompositionModule_LSTM(slayers=slayers, mlayers=mlayers, channel=channel))\n",
        "        for m in self.modules():\n",
        "            # 也可以判断是否为conv2d，使用相应的初始化方式\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                #nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, D):\n",
        "        T = torch.zeros(D.shape).to(D.device)\n",
        "        [h, c] = [None, None]\n",
        "        # B = torch.zeros(D.shape).to(D.device)\n",
        "        for i in range(self.stage_num):\n",
        "            D, T, h, c = self.decos[i](D, T, h, c)\n",
        "        if self.mode == 'train':\n",
        "            return D,T\n",
        "        else:\n",
        "            return T\n",
        "\n",
        "class DecompositionModule_LSTM(nn.Module):\n",
        "    def __init__(self, slayers=6, mlayers=3, channel=32):\n",
        "        super(DecompositionModule_LSTM, self).__init__()\n",
        "        self.lowrank = LowrankModule_LSTM(channel=channel)\n",
        "        self.sparse = SparseModule_LSTM(channel=channel, layers=slayers)\n",
        "        self.merge = MergeModule_LSTM(channel=channel, layers=mlayers)\n",
        "\n",
        "    def forward(self, D, T, h, c):\n",
        "        B, h, c = self.lowrank(D, T, h, c)\n",
        "        T = self.sparse(D, B, T)\n",
        "        D = self.merge(B, T)\n",
        "        return D, T, h, c\n",
        "\n",
        "class LowrankModule_LSTM(nn.Module):\n",
        "    def __init__(self, channel=32):\n",
        "        super(LowrankModule_LSTM, self).__init__()\n",
        "        self.conv1_C = nn.Sequential(nn.Conv2d(1, channel, kernel_size=3, padding=1, stride=1),\n",
        "                         nn.BatchNorm2d(channel),\n",
        "                         nn.ReLU(True))\n",
        "        self.RB_1 = ResidualBlock(channel, channel, 3, bias=True, res_scale=1)\n",
        "        self.RB_2 = ResidualBlock(channel, channel, 3, bias=True, res_scale=1)\n",
        "        self.convC_1 = nn.Conv2d(channel, 1, kernel_size=3, padding=1, stride=1)\n",
        "        self.ConvLSTM = ConvLSTM(channel, channel, 3)\n",
        "\n",
        "    def forward(self, D, T, h, c):\n",
        "        x = D - T\n",
        "        x_c = self.conv1_C(x)\n",
        "        x_c1 = self.RB_1(x_c)\n",
        "        x_ct, h, c = self.ConvLSTM(x_c1, h, c)\n",
        "        x_c2 = self.RB_2(x_ct)\n",
        "        x_1 = self.convC_1(x_c2)\n",
        "        B = x + x_1\n",
        "        return B, h, c\n",
        "\n",
        "class SparseModule_LSTM(nn.Module):\n",
        "    def __init__(self, channel=32, layers=6) -> object:\n",
        "        super(SparseModule_LSTM, self).__init__()\n",
        "        convs = [nn.Conv2d(1, channel, kernel_size=3, padding=1, stride=1),\n",
        "                 nn.ReLU(True)]\n",
        "        for i in range(layers):\n",
        "            convs.append(nn.Conv2d(channel, channel, kernel_size=3, padding=1, stride=1))\n",
        "            convs.append(nn.ReLU(True))\n",
        "        convs.append(nn.Conv2d(channel, 1, kernel_size=3, padding=1, stride=1))\n",
        "        self.convs = nn.Sequential(*convs)\n",
        "\n",
        "        self.epsilon = nn.Parameter(torch.Tensor([0.01]), requires_grad=True)\n",
        "        self.contrast = nn.Sequential(\n",
        "                nn.Conv2d(1, channel, kernel_size=3, padding=1, stride=1),\n",
        "                nn.ReLU(True),\n",
        "                AttnContrastLayer_n(channel, kernel_size=17, padding=8),\n",
        "                nn.BatchNorm2d(channel),\n",
        "                nn.LeakyReLU(0.1, inplace=True),\n",
        "                nn.Conv2d(channel, 1, kernel_size=3, padding=1, stride=1)\n",
        "            )\n",
        "\n",
        "\n",
        "    def forward(self, D, B, T):\n",
        "        x = T + D - B\n",
        "        w = self.contrast(x)\n",
        "        T = x - self.epsilon * self.convs(x + w)\n",
        "        return T\n",
        "\n",
        "class MergeModule_LSTM(nn.Module):\n",
        "    def __init__(self,  channel=32, layers=3):\n",
        "        super(MergeModule_LSTM, self).__init__()\n",
        "        convs = [nn.Conv2d(1, channel, kernel_size=3, padding=1, stride=1),\n",
        "                 nn.BatchNorm2d(channel),\n",
        "                 nn.ReLU(True)]\n",
        "        for i in range(layers):\n",
        "            convs.append(nn.Conv2d(channel, channel, kernel_size=3, padding=1, stride=1))\n",
        "            convs.append(nn.BatchNorm2d(channel))\n",
        "            convs.append(nn.ReLU(True))\n",
        "        convs.append(nn.Conv2d(channel, 1, kernel_size=3, padding=1, stride=1))\n",
        "        self.mapping = nn.Sequential(*convs)\n",
        "\n",
        "    def forward(self, B, T):\n",
        "        x = B + T\n",
        "        D = self.mapping(x)\n",
        "        return D"
      ],
      "metadata": {
        "id": "nrFHiEekr9at"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ハイパーパラメータ\n",
        "batch_size = 8\n",
        "lr = 1e-4\n",
        "num_epochs = 50\n",
        "\n",
        "# ---- データローダ ----\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = NEUSegDataset(\n",
        "    base_dir=\"/content/datasets/NEU_Seg_Custom\",\n",
        ")\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "# ---- モデル ----\n",
        "model = RPCANet9(stage_num=6)  # 必要に応じてステージ数を調整\n",
        "model.to(device)\n",
        "\n",
        "# ---- 損失と最適化 ----\n",
        "\n",
        "# 再構成誤差として L2 Loss\n",
        "recon_loss_fn = nn.MSELoss()\n",
        "\n",
        "# オプティマイザ\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# ---- 訓練ループ ----\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "        inputs = data[0].to(device)\n",
        "\n",
        "        # Forward: low-rank + sparse component\n",
        "        # RPCANet では復元背景 & スパース部分を出力する構造があるはずです\n",
        "        # 例: low_rank, sparse, recon = model(inputs)\n",
        "        low_rank, sparse = model(inputs)\n",
        "\n",
        "        # 再構成：\n",
        "        recon = low_rank + sparse\n",
        "\n",
        "        # 再構成誤差\n",
        "        loss_recon = recon_loss_fn(recon, inputs)\n",
        "\n",
        "        # スパース部の正則化（L1 など）\n",
        "        loss_sparse = torch.mean(torch.abs(sparse))\n",
        "\n",
        "        # 総合損失\n",
        "        loss = loss_recon + 0.1 * loss_sparse\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {avg_loss:.6f}\")\n",
        "\n",
        "    # モデルの保存\n",
        "    checkpoint_path = f\"checkpoints/rpcanet_anomaly_epoch{epoch+1}.pth\"\n",
        "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "print(\"Training Completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHew36QHoyZI",
        "outputId": "1fa95907-55a9-4d58-9e6c-7c3532f63b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 推論と結果の可視化\n",
        "model.eval()\n",
        "test_images = next(iter(train_loader)).to(device)\n",
        "with torch.no_grad():\n",
        "    L, S = model(test_images)\n",
        "\n",
        "# 最初の1枚を表示\n",
        "idx = 0\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(test_images[idx].cpu().squeeze(), cmap='gray')\n",
        "plt.title(\"Original (Metal Surface)\")\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(L[idx].cpu().squeeze(), cmap='gray')\n",
        "plt.title(\"Low-Rank (Background)\")\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(torch.abs(S[idx]).cpu().squeeze(), cmap='hot')\n",
        "plt.title(\"Sparse (Detected Defect)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0_r1O9NUnzu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5etosJGwn0Ub"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}