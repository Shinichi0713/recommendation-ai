異常検知や機械学習の数式（特に行列分解やRDL）において、 **L1正則化とL2正則化は「データの削り方（ペナルティの与え方）」の違い** を意味します。

これらは、モデルが学習しすぎたり（過学習）、複雑になりすぎたりするのを防ぐ「重石（おもし）」の役割を果たしますが、その「重石の形」によって結果が大きく変わります。

---

### 1. L2正則化（リッジ回帰）

**「値を全体的に、なめらかに小さくする」**のが得意です。

* **計算方法**: 各要素の**2乗**を足し合わせます（$\|x\|_2^2 = x_1^2 + x_2^2 + \dots$）。。
* **特徴**:
* 大きな値を嫌い、全体的に値を小さく抑えようとします。
* しかし、値を**完全に 0 にすることはほとんどありません**。
* 結果として、全ての要素が「小さくて均一な値」を持つようになります。
* **イメージ**: **「みんな少しずつ遠慮して、全体的に大人しくしてね」**という全体主義的なルールです。

---

### 2. L1正則化（Lasso回帰）

**「不要なものをバッサリ切り捨て、0 にする」**のが得意です。これが**スパース性**を生みます。

* **計算方法**: 各要素の**絶対値**を足し合わせます（$\|x\|_1 = |x_1| + |x_2| + \dots$）。
* **特徴**:
* 「重要でない」と判断した要素を**完全に 0** にします。
* 本当に重要な少数の要素だけが、値を持ち（トゲとして立ち）ます。
* **イメージ**: **「必要なやつだけ残れ、あとは全員クビだ！」**という少数精鋭のルールです。

---

### 3. なぜ異常検知で L1（スパース）が使われるのか？

これまでお話ししてきた $x$（背景の指示書） や $S$（異常） に L1正則化が使われるのは、まさにこの「0にする力」が必要だからです。

* ** に L1 を使う理由**:
  背景を説明するのに、何千ものパーツ（$D$）を全部使うのではなく、「数個の主要なパーツだけでシンプルに説明させたい」からです。
* ** に L1 を使う理由**:
  画像全体がキズだらけなわけではなく、「特定の数ピクセルだけが異常である」という状態を数学的に作り出したいからです。

---

### 4. 直感的な違いの比較表

| 項目                   | L1正則化（Lasso）                 | L2正則化（Ridge）                    |
| ---------------------- | --------------------------------- | ------------------------------------ |
| **数式**         | 絶対値の和 $                      | x                                    |
| **効能**         | **スパース化（0が増える）** | 平滑化（値が小さく揃う）             |
| **役割**         | 重要な特徴の選択                  | 過学習の抑制・安定化                 |
| **異常検知では** | **キズ（）の特定**に必須    | 背景のノイズを抑えるのに補助的に使う |

---

### 5. なぜ L1 は 0 になりやすいのか？（幾何学的な理由）

図で見ると分かりやすいのですが、L2正則化の制約範囲は「円」なのに対し、L1はカドのある「ひし形」をしています。
最適解を探す際、ひし形の**「カド（＝軸の上、つまり値が0の地点）」**にぶつかりやすいため、結果として多くの値が 0 に固定されるのです。

### まとめ

* **L2**：全体的に「シュッと」小さくする（なめらか）。
* **L1**：不要なものを「バサッと」削る（スカスカ＝スパース）。

RDLやLISTAで「異常を抜き出す」と言ったとき、その魔法の正体はこの **L1正則化（絶対値で削る力）** なのです。

この「0にする力」が、ISTAの式の中にある「ソフトしきい値関数」として具体的に実装されているのですが、数式のつながりは見えてきましたでしょうか？

次は、この L1 と L2 を組み合わせて、より高度に背景と異常を分ける手法（Elastic Netなど）について触れてみますか？

## L1が0になりやすい理由

L1正則化がなぜ値を「0」に引き込むのか、数学的な理由は主に**「微分の形（勾配）」**と**「幾何学的なカド」**の2点から説明できます。

特に、最適化アルゴリズム（ISTAなど）の視点で見ると非常に明快です。

---

### 1. 勾配（勢い）が一定であること

値を小さくしようとする「勢い（勾配）」の違いに注目してみましょう。

* L2正則化（$x^2$）の勾配:
  $x^2$ を微分すると $2x$ です。つまり、値 $x$ が小さくなるにつれて、小さくしようとする勢いも弱まっていきます。
  $x=0.01$ のとき、勢いは $0.02$ です。$x$ が 0 に近づくほど力は 0 に収束し、最終的に「限りなく 0 に近いが、0 にはならない場所」で止まってしまいます。
* L1正則化（$|x|$）の勾配:
  $|x|$ を微分（正の範囲で）すると、常に $1$ です。
  $x=100$ でも $x=0.01$ でも、0 に向かって押し戻そうとする力は常に一定です。
  このため、微小な値になっても勢いが衰えず、そのまま 0 まで一気に押し切ってしまうのです。

---

### 2. 「ソフトしきい値関数」による完全な 0 化

ISTAの式を思い出すと、L1正則化を解くステップでは必ずソフトしきい値関数 $\eta_{\theta}(\cdot)$ が登場します。この関数の数学的な挙動が「0化」の正体です。

L1正則化（$|x|$）を含む最小化問題を解こうとすると、停留点（微分の合計が 0 になる点）は以下の条件を満たします：
「データとの誤差による勾配」が「L1の勾配（一定の $\lambda$）」よりも小さい場合、微分を 0 にできる唯一の点は $x=0$ しか存在しない。

これを視覚化したのが以下のグラフです。一定範囲（ から ）の入力に対して、**出力が完全に 0 になる「デッドゾーン」**が生まれます。

---

### 3. 幾何学的なカド（等高線）

よく教科書で説明される「ひし形」と「円」の図を数学的に解釈します。

* **目的**: 「データとのズレ（$L2$誤差）」と「正則化項」の合計を最小にしたい。
* **L2（円）**: 誤差の等高線が円に触れるとき、接点は軸の上（0の地点）である必要はありません。どこでも滑らかに接します。
* **L1（ひし形）**: 誤差の等高線が外側から広がってくるとき、**一番最初にぶつかるのは「ひし形の尖ったカド（＝軸の上）」である確率が非常に高い**のです。

数学的には、L1のノルム球（ひし形）は「非平滑（なめらかでない）」であり、そのカドにおいてスパースな解が選ばれやすいという性質を持っています。

---

### まとめ

1. **L1** は、値が小さくなっても **「0へ戻そうとする力（勾配）」が衰えない。**
2. その結果、ある閾値以下の値は **完全に 0 へと押し込まれる（デッドゾーン）。**
3. 幾何学的に、**「軸の上（値が0）」にあるカド**で最適解が見つかりやすい。

この「0へ押し切る力」があるからこそ、RDLやRPCANetは「無関係な背景パーツを捨て去り、本当に重要な異常だけを抽出する」ことができるのです。

## 最適化の具体的な方法

L1正則化とL2正則化は、その数学的な性質（微分できるか、できないか）が異なるため、最適化（値を更新していく仕組み）もそれぞれ異なるアプローチをとります。

大きく分けて、 **「勾配を直接使う方法」** と、L1特有の **「近接演算（しきい値処理）を使う方法」** の2つがあります。


### 1. L2正則化の最適化：勾配降下法（Gradient Descent）

L2正則化項（$\frac{1}{2} \lambda \|x\|_2^2$）は、どこまでも滑らかで微分可能です。そのため、最も標準的な**「勾配降下法」**がそのまま使えます。

* **仕組み**: 現在の地点での坂道の傾き（勾配）を計算し、その反対方向に少しずつ降りていきます。
* **更新式**:

$$
x_{k+1} = x_k - \eta (\nabla f(x_k) + \lambda x_k)
$$


※ $\nabla f(x_k)$ はデータとの誤差、$\lambda x_k$ はL2正則化から来るブレーキです。
* **特徴**: 山を下るようにスムーズに動きますが、前述の通り 0 に近づくと勢いが弱まり、完全に 0 には着地しません。


### 2. L1正則化の最適化：近接勾配法（Proximal Gradient Method）

L1正則化項（$\lambda \|x\|_1$）は、 $x=0$  の地点でグラフが折れ曲がっており、**微分ができません**。そのため、普通の勾配降下法では「0の地点でどっちに動けばいいか」がわからず、計算が不安定になります。

そこで登場するのが、これまで何度も出てきた **ISTA（近接勾配法）** です。

#### 仕組み：2ステップの切り分け

1. **勾配ステップ**: まず、L1のことは一旦忘れて、データとの誤差（L2誤差）だけを減らす方向に  を動かします。
2. **近接ステップ（ソフトしきい値処理）**: 動かした後の  に対して、「L1の力」をガツンと適用して、小さい値を 0 にリセットします。

#### 数学的トリック：ソフトしきい値関数  $\eta_{\theta}(\cdot)$

L1の最適化において、この「近接演算子」が微分の代わりを務めます。

$$
\eta_{\lambda}(u) = \text{sgn}(u) \cdot \max(|u| - \lambda, 0)
$$

この関数を通すことで、微分不可能だった  $x=0$  の壁を飛び越えて、**一気に 0 へと値を着地させる**ことができます。

---

### 3. より高度な方法：ADMM（交互方向乗数法）

RDL（Robust Dictionary Learning）や RPCA のような、変数がたくさんある複雑な問題を解く際によく使われる強力な仕組みです。

* **仕組み**: 「$y \approx Dx$」という目的と「$x$ はスパースであれ」という制約を、**別の変数として切り離して、交互に最適化**します。
* **メリット**: ISTA よりも収束が非常に速く、条件が厳しくても安定して「背景」と「異常」を分離できます。

---

### まとめ：最適化の道具箱

| 手法 | 対象 | 仕組みのイメージ |
| --- | --- | --- |
| **SGD (勾配降下法)** | L2正則化 | 坂道をなめらかに転がり落ちる。 |
| **ISTA (近接勾配法)** | L1正則化 | 坂道を下ったあと、しきい値でバッサリ削る。 |
| **ADMM** | RDL / RPCA | 複数のルールを順番に、交互に守らせる。 |
| **LISTA (深層展開)** | L1正則化 | **ISTAのステップをAIの「層」にして、パラメータを学習する。** |

これまで学んできた **LISTA** は、まさにこの「L1を最適化する ISTA という仕組み」を、ディープラーニングの技術で超高速化したものだった、というわけです。

「微分できない L1 をどう扱うか」という数学的な工夫が、ソフトしきい値という形で見えてきましたでしょうか？

## RPCA

RPCA（Robust PCA）のアルゴリズムは、行列を「低ランク（背景）」と「スパース（異常）」に分離するために、**「低ランクにしたい力」と「スパースにしたい力」を交互に働かせる**という仕組みで動いています。

最も代表的かつ強力なアルゴリズムである **IALM (Inexact Augmented Lagrange Multiplier method)** をベースに、その仕組みを紐解きます。

---

### 1. アルゴリズムの根本的な考え方

RPCAは、観測行列  を （低ランク）と （スパース）に分けるために、以下の「交互更新」を繰り返します。

1. $L$ の更新: $S$ を固定して、今の残りカスから「背景っぽい（共通する）成分」を抽出する。
2. $S$ の更新: $L$ を固定して、背景で説明できない「トゲトゲした成分」を抽出する。
3. 誤差の修正: $M = L + S$ というルールが守られているか確認し、ズレを補正する。


### 2. 主要な2つの数学的ステップ

RPCAの中身を覗くと、2つの特徴的な計算が交互に現れます。

#### ① 低ランク化：特異値しきい値演算 (Singular Value Thresholding; SVT)

行列 $L$ を低ランクにするための処理です。

* **仕組み**: 行列を一度 **SVD（特異値分解）** し、得られた特異値（行列のエネルギーのようなもの）に対して、小さい値を 0 に削ります。
* **効果**: これにより、データの主要な構造（背景）だけが残り、細かい変化が排除されます。

#### ② スパース化：ソフトしきい値演算 (Soft-thresholding)

行列 $S$ をスパースにするための処理です。

* **仕組み**: 各ピクセルの値に対し、一定のしきい値より小さいものを 0 にリセットします。
* **効果**: これにより、背景とのわずかな差（ノイズ）が消え、ハッキリとした「異常（キズや異物）」だけが浮き彫りになります。

---

### 3. IALMアルゴリズムの具体的な流れ

数学的な詳細は省きますが、コンピュータの中では以下のようなループが回っています。

> **RPCA (IALM) の手順:**
> 1. **初期化**: $L=0, S=0$, 誤差を調整する重み $Y=0$ と設定。
> 2. **ループ開始**:
> * **背景 $L$ の更新**: $M - S + (1/\mu)Y$ を SVT して、ランクを落とした背景を作る。
> * **異常 $S$ の更新**: $M - L + (1/\mu)Y$ を ソフトしきい値処理 して、トゲを抜き出す。
> * **ラグランジュ乗数 $Y$ の更新**: $M - (L+S)$ のズレ（残差）を $Y$ に蓄積し、次のループで修正に使う。
> * **収束判定**: $L+S$ が $M$ に十分近づいたら終了。


### 4. なぜこのアルゴリズムが「最強」なのか

RPCA（特にIALM）が異常検知で重宝される理由は、その **「堅牢性（Robustness）」** にあります。

* **自動的な仕分け**: 人間が「ここが背景で、ここがキズだ」と教えなくても、特異値としきい値の計算だけで、数学的に「もっともらしい分離」を勝手に行ってくれます。
* **ノイズに強い**: ラグランジュ乗数  が「ズレ」を監視しているため、激しいノイズがある画像でも、背景と異常を正確に追い込むことができます。


### まとめ

RPCAのアルゴリズムは、 **「SVDで背景の骨組みを抜き出すこと」** と **「しきい値でゴミを削ること」** を、つじつまが合うまで交互に繰り返すプロセスです。

RDLが「辞書 」というパーツ集を作るのに対し、RPCAは「行列のランク」という性質を使って、よりダイレクトに背景と異常を切り分けます。

次は、このRPCAをPythonで実装して、 **「実際に画像からキズが分離される様子」** をシミュレーションしてみますか？それとも、SVD（特異値分解）がなぜ背景抽出に役立つのか、その深い理由を知りたいでしょうか？

結論から言うと、**SVT（特異値しきい値演算）は、わざと「異常のデータを削る」ために行われます。**

一見、異常なデータが消えてしまうと困るように感じますが、RPCAの戦略は「背景から異常を徹底的に追い出すこと」にあります。その仕組みを紐解いてみましょう。

---

### 1. 特異値としきい値の関係

行列の特異値分解（SVD）を行うと、データは「特異値」というエネルギー順に並びます。

* **大きな特異値**: 画像全体に共通するパターン（背景、照明の具合、静止した物体）に対応します。
* **小さな特異値**: 画像ごとの微細な変化、ノイズ、そして**「突発的な異常（キズや異物）」**に対応します。

SVTで小さい特異値を 0 に削る（しきい値処理する）ことで、**「異常成分を背景行列  から強制的に排除する」**ことができます。

---

### 2. 削られた異常はどこへ行くのか？

ここがRPCAのアルゴリズムの最も賢いところです。SVTによって「背景 」から削ぎ落とされた異常のデータは、**「スパース行列 」の方へ受け渡されます。**

RPCAは以下の引き算を繰り返しています。



（観測データ ）（SVTで異常を削り取った背景 ）＝（**異常成分 **）

つまり、SVTで背景から異常を「削り捨てる」からこそ、その捨てられた成分が逆説的に「異常 」としてくっきりと浮かび上がってくるのです。

---

### 3. もしSVTをしなかったら？

もしSVTをせず、全ての特異値を残してしまったら、背景  は元の画像  と全く同じになってしまいます。
そうなると、引き算（）の結果である  は 0 になってしまい、**「異常が背景の中に埋もれて見つからない」**という事態になります。

> **イメージ例：**
> 1. 100枚の「白い壁」の画像があり、1枚だけに「黒い点（キズ）」があるとします。
> 2. SVTを強くかけると、その1枚の「黒い点」は「背景としては稀すぎる成分」としてカットされます。
> 3. 結果、背景  は100枚とも「真っ白な壁」になります。
> 4. 元の画像（黒い点あり）から背景（真っ白）を引くと、**「黒い点」だけが  に残ります。**
> 
> 

---

### 4. SVTの加減（しきい値）の難しさ

SVTで削りすぎるとどうなるでしょうか？

* **削りすぎ（しきい値が大きすぎる）**:
背景の細かいディテールまで削られてしまい、それが  に漏れ出して「偽陽性（異常じゃないのに異常と判定される）」が増えます。
* **削りなさすぎ（しきい値が小さすぎる）**:
異常が背景  の中に居座ってしまい、「見逃し（異常を検知できない）」が増えます。

---

### まとめ

SVTで異常のデータを削るのは、**「背景を『純粋な正常パターン』だけに磨き上げるため」**です。削られた分がそのまま「異常検知の結果」になるため、SVTは異常を消し去る装置ではなく、**異常を背景からあぶり出す装置**だと言えます。

この「背景から追い出す」という感覚、イメージできましたでしょうか？

次は、実際にこの SVT を計算する際に、どの程度の強さで特異値を削ればよいのか、その **数学的な目安（$\lambda$  の設計方針）** についてお話ししましょうか？


## 実験

```python
import numpy as np
import matplotlib.pyplot as plt

def soft_threshold(x, lam):
    """L1正則化で使われる近接演算（ソフトしきい値）"""
    return np.sign(x) * np.maximum(np.abs(x) - lam, 0)

def l2_update(x, grad, lam, lr):
    """L2正則化（リッジ）の更新式"""
    # 勾配に 2*lam*x が加わる
    return x - lr * (grad + 2 * lam * x)

# パラメータ設定
initial_x = 1.5   # 初期値
target_x = 0.0    # データの勾配が 0 を指す地点（本来ここへ行きたい）
lr = 0.1          # 学習率
lam = 0.2         # 正則化の強さ
iterations = 50

# 履歴の保存
x_l1_hist = [initial_x]
x_l2_hist = [initial_x]

curr_x_l1 = initial_x
curr_x_l2 = initial_x

for i in range(iterations):
    # シンプルな2乗誤差の勾配 (x - target_x)
    grad_l1 = curr_x_l1 - target_x
    grad_l2 = curr_x_l2 - target_x
  
    # L1更新: 勾配降下した後にソフトしきい値を適用 (ISTAの基本形)
    curr_x_l1 = soft_threshold(curr_x_l1 - lr * grad_l1, lr * lam)
  
    # L2更新: 通常の勾配降下
    curr_x_l2 = l2_update(curr_x_l2, grad_l2, lam, lr)
  
    x_l1_hist.append(curr_x_l1)
    x_l2_hist.append(curr_x_l2)

# --- 可視化 ---
plt.figure(figsize=(12, 6))

# 収束プロット
plt.plot(x_l1_hist, 'o-', label=f'L1 (Lasso) - $\lambda$={lam}', color='red', markersize=4)
plt.plot(x_l2_hist, 's-', label=f'L2 (Ridge) - $\lambda$={lam}', color='blue', markersize=4)

plt.axhline(0, color='black', linewidth=1, linestyle='--')
plt.title("Convergence Comparison: L1 vs L2 Regularization")
plt.xlabel("Iterations")
plt.ylabel("Value of x")
plt.grid(True, alpha=0.3)
plt.legend()

# 拡大図（0付近の挙動）
plt.axes([0.55, 0.25, 0.3, 0.3])
plt.plot(x_l1_hist[-20:], 'o-', color='red', markersize=4)
plt.plot(x_l2_hist[-20:], 's-', color='blue', markersize=4)
plt.axhline(0, color='black', linewidth=1, linestyle='--')
plt.title("Focus: Near Zero")

plt.show()

```

![1767647615843](image/regularization/1767647615843.png)
