ニューラルネットワーク（深層学習）を用いた異常検知は、これまで議論してきた数学的な手法（RPCAやRX）と比べて、 **「表現力の高さ」** と **「柔軟性」** において圧倒的なアドバンテージがあります。

主な特徴を、代表的な手法のメカニズムと共に解説します。

---

### 1. 特徴抽出の自動化（Representation Learning）

数学的手法では「パッチ分割」や「ハンケル行列化」などの人間による設計（手作業）が必要でしたが、ニューラルネットワークはデータから直接、最適な特徴を抽出します。

* **特徴**: 画素の生の数値だけでなく、エッジ、質感、さらには「物体としての意味」を階層的に理解します。
* **メリット**: 人間が「キズとはこういうものだ」と定義しなくても、正常データの複雑なルールを勝手に学習してくれます。

---

### 2. 代表的な3つのアプローチ

#### ① 再構成誤差ベース (AutoEncoder / GAN)

正常データだけを学習し、入力を再現するように訓練します。

* **仕組み**: 正常な画像はきれいに復元できますが、未知の異常箇所は復元に失敗します（再構成誤差が大きくなる）。
* **進化形**: あなたが以前触れた **CDR** や **LISTA** も、この「効率的な復元」を追求した形の一種です。

#### ② 特徴空間での分布モデリング (Deep SVDD / NF)

データを高次元の特徴空間に写像し、正常データが「一つの球」や「特定の分布」に収まるように学習します。

* **Deep SVDD**: 正常データを中心点に凝縮させます。中心から遠いものを異常と判定します。
* **Normalizing Flows (NF)**: 複雑な画像分布を単純なガウス分布に変換します。変換後の確率密度が低い場所を異常とします。

#### ③ 自己教師あり学習 (Self-Supervised Learning)

画像の一部を隠したり、回転させたりして「元の状態を当てる」クイズを解かせます。

* **特徴**: 「何が正常か」をより深く理解させるため、パズルを解かせることでデータの構造を学習させます。異常なデータはこのパズルのルールに従わないため、検知が可能になります。

---

### 3. 数学的手法（RPCA等）と比較したメリット・デメリット

| 特徴 | 数学的手法 (RPCA/RX) | ニューラルネットワーク (DL) |
| --- | --- | --- |
| **学習データ** | 少量、あるいは不要（教師なし） | 大量のデータが必要 |
| **計算速度** | 推論（SVD等）が重い | 学習は重いが、**推論は爆速** |
| **解釈性** | なぜ異常かが数学的に明確 | ブラックボックスになりがち |
| **複雑な背景** | 限界がある | 極めて複雑な背景も学習可能 |

---

### 4. 現代のトレンド：ハイブリッド手法

最近の実務では、今回議論してきた「数学の知恵」と「ニューラルネットワークのパワー」を融合させるのが最強とされています。

* **Deep RX**: CNNで抽出した高度な特徴量に対して、マハラノビス距離（RX）を適用する。
* **Deep展開 (Unrolling)**: **LISTA** のように、ISTAのアルゴリズム構造そのものをニューラルネットワークの層にしてしまう。

---

### まとめ：なぜニューラルネットワークを使うのか？

最大の理由は、**「RPCAやRXでは捉えきれない、高次で抽象的な『正常さ』をモデル化できるから」**です。

例えば、単純な「明るさの変化（RPCAが得意）」ではなく、「部品の形がわずかに歪んでいる」「質感がザラついている」といった**感覚的な異常**を捉えるには、ニューラルネットワークの表現力が不可欠です。

次は、これまで実装した「RPCA + RX」のロジックを、ニューラルネットワークの「特徴抽出器（Pre-trained Model）」と組み合わせて、さらに精度を高める方法について詳しく見てみますか？


## 柔軟性に優れる理由

ニューラルネットワークがRPCAやRXなどの古典的な数学的手法よりも**「表現力」と「柔軟性」**に優れている理由は、主に**「非線形性」「階層構造」「学習による最適化」**という3つのポイントに集約されます。

わかりやすく紐解いてみましょう。

---

### 1. 非線形な「境界線」が引ける

RPCAやRXは、数学的には「線形（まっすぐな関係）」に基づいています。

* **古典的手法**: 「データの広がりはだいたい楕円形（ガウス分布）だろう」と仮定して、そこから外れたものを探します。
* **ニューラルネットワーク**: **活性化関数（ReLUなど）**という仕組みにより、グニャグニャと曲がった複雑な境界線を作れます。

**なぜこれが重要か？**
現実の画像（例えば「本物の大理石の模様」）の分布は、綺麗な楕円形ではなく、非常に複雑な形をしています。ニューラルネットワークはこの複雑な形にぴったり沿うように「正常の範囲」を定義できるため、誤検知が少なくなります。

---

### 2. 階層的な「理解」ができる

ここが最大の特徴です。ニューラルネットワーク（特にCNN）は、情報を段階的に抽象化して理解します。

* **浅い層**: 点、線、色などの「局所的な特徴」を捉える。
* **深い層**: 質感、模様、パーツの配置といった「全体的な意味」を捉える。

**なぜこれが重要か？**
RPCAは「画素の数値」を直接いじりますが、ニューラルネットワークは**「この位置にこの模様があるのは、製品の構造上おかしい」**という、数値だけでは説明できない「意味的な異常」を捉えることができます。

---

### 3. データに合わせて「自分を改造」できる

数学的手法は、あらかじめ決められた計算式（アルゴリズム）にデータを流し込みます。一方、ニューラルネットワークはデータを見て、自分の中のパラメータ（数億個の重み）を書き換えます。

* **柔軟性**:
* 金属の表面検査なら、金属特有の光沢を無視するように学習する。
* 布の検品なら、糸の編み目の周期性を理解するように学習する。


* **最適化**: 目的（異常を検知すること）を達成するために、どの特徴を重視し、どのノイズを無視すべきかを「逆伝播（Backpropagation）」によって自動で見つけ出します。

---

### 4. 「多次元の相関」を丸ごと扱える

RX Detectorは共分散行列を使って変数間の関係を見ますが、変数の数（次元数）が増えると計算が破綻（次元の呪い）しやすくなります。

ニューラルネットワークは、数千・数万次元のデータであっても、**「注意（Attention）」**などの仕組みを使って、「どの画素とどの画素が関連し合っているか」という巨大な相関関係を、効率よく、かつ柔軟に学習できます。

---

### まとめ：何が違うのか？

| 特性 | 古典的手法（RPCA/RX） | ニューラルネットワーク |
| --- | --- | --- |
| **捉え方** | 統計的な「外れ値」を探す | データの「本質的な形」を模写する |
| **得意不得意** | 単純な背景やノイズに強い | 複雑なテクスチャや意味的な異常に強い |
| **知識の源** | 人間が与えた数学モデル | 大量のデータから学んだ経験 |

### 次のステップへの視点

ニューラルネットワークのこの「表現力」を最大限に引き出すのが、まさにこれまでお話しした **LISTA** です。LISTAは、 **「数学的なISTAの正しさ」を「ニューラルネットワークの表現力」で包み込んだ** ハイブリッドな存在と言えます。

## そのほか手法

Neural Network（深層学習）を用いた異常検知は、近年非常に進化しており、データの種類（画像、信号、数値）や「何をもって異常とするか」の定義によって、いくつかの大きなアプローチに分かれます。

主要な手法を、その仕組みと特徴で整理して解説します。


### 1. 再構成誤差ベース（Reconstruction-based）

「正常データは完璧に復元できるが、異常データは復元に失敗する」という性質を利用します。

* **AutoEncoder (AE) / Variational AutoEncoder (VAE)**:
入力を一度ギュッと圧縮（次元圧縮）し、再び元の画像に戻すように学習します。異常な箇所はネットワークにとって「未知」のため、うまく復元できず、元の画像との差分（誤差）が大きくなります。
* **GAN (Generative Adversarial Networks)**:
正常データだけを生成するように学習したGAN（AnoGANなど）を使用します。入力画像に最も近い「正常な画像」をGANに作らせ、その差分を異常と見なします。


### 2. 距離・分布ベース（Embedding & Distribution-based）

データを特徴空間上のベクトルに変換し、その「位置」で判定します。

* **Deep SVDD (Deep Support Vector Data Description)**:
ニューラルネットワークを使い、全ての正常データを特徴空間上の「一つの小さな球」の中に閉じ込めるように学習します。この球の外側にプロットされたものを異常とします。
* **Normalizing Flows (NF)**:
複雑なデータの分布を、数学的に扱いやすい単純な分布（ガウス分布など）に変換する手法です。変換後の確率密度が低いデータは「稀なもの＝異常」と判定されます。


### 3. 特徴抽出 + 統計ベース（Hybrid Approach）

今回お話しした **Deep RX** や **PaDiM** がここに含まれます。

* **Deep RX / PaDiM**:
学習済みのCNN（ResNet等）を「巨大な特徴抽出器」として使い、得られた特徴マップに対してマハラノビス距離などの統計的判定を行います。
* **PatchCore**:
正常なパッチの特徴量を「メモリバンク」として全て保存しておき、新しいデータがそのメモリ内のどれとも似ていなければ異常とする手法です。現在の画像異常検知における最高峰の手法の一つです。


### 4. 自己教師あり学習（Self-Supervised Learning）

データ自体にクイズを作らせて、データの構造を深く理解させます。

* **Rotation Prediction**: 「この画像は何回回転しているか？」を当てさせる学習です。正常な構造を理解していないと解けないため、異常なデータが入るとクイズの正解率が下がることを利用します。
* **CutPaste**: 画像の一部を切り取って別の場所に貼り付け、その「違和感」を検知できるように学習させます。


### 5. 時系列・信号向け（Temporal / Sequential）

時間の流れ（コンテキスト）の異常を捉えます。

* **LSTM / GRU / Transformer (Prediction-based)**:
「過去のデータから次の一点を予測する」ように学習します。予測値と実際の値が大きくズレた場合、そこを異常（突発的な変化）と見なします。
* **Deepを展開した手法 (LISTAなど)**:
以前お話しした通り、スパースモデリングのアルゴリズムをNNの層として構成し、高速かつ高精度に信号を分解・抽出します。


**Rotation Prediction**（回転予測）は、異常検知における「自己教師あり学習（Self-Supervised Learning）」の代表的な手法です。

「正常な物体には決まった向きや構造がある」という前提を利用し、画像をあえて回転させて**「何度回転させたか？」を当てるクイズ**を解かせることで、モデルに正常データの構造を深く理解させます。

---

### 1. 実装のコンセプト

1. **正常画像のみ**を準備します。
2. 各画像を 0°, 90°, 180°, 270° の4パターンに回転させます。
3. モデル（CNN）に画像を入力し、**「0, 1, 2, 3」の4クラス分類**を解かせます。
4. **異常検知の仕組み**:
* 正常な画像であれば、モデルは正しい回転角度を予測できます（確信度が高い）。
* 異常な画像（キズや異物がある、あるいは全く別の物体）の場合、モデルにとって未知のパターンが含まれるため、回転角度の予測に失敗、あるいは確信度が低くなります。



---

### 2. PyTorchによる実装コード

以下に、学習から推論までのエッセンスを凝縮したコードを示します。

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import torchvision.transforms.functional as TF

# 1. 回転予測用モデルの定義
class RotationNet(nn.Module):
    def __init__(self):
        super(RotationNet, self).__init__()
        # 特徴抽出器としてResNet18を使用
        self.backbone = models.resnet18(pretrained=True)
        # 最終層を4クラス分類（0, 90, 180, 270度）に変更
        num_ftrs = self.backbone.fc.in_features
        self.backbone.fc = nn.Linear(num_ftrs, 4)

    def forward(self, x):
        return self.backbone(x)

# 2. 学習ループ（正常データのみを使用）
def train_rotation_net(model, dataloader, epochs=10):
    optimizer = optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.CrossEntropyLoss()
    model.train()

    for epoch in range(epochs):
        for inputs, _ in dataloader: # ラベルは無視
            # 1つの画像から4つの回転バリエーションを作成
            rotated_images = []
            labels = []
            for angle_idx, angle in enumerate([0, 90, 180, 270]):
                rotated_img = TF.rotate(inputs, angle)
                rotated_images.append(rotated_img)
                labels.append(torch.full((inputs.size(0),), angle_idx))
            
            # まとめてバッチ化
            inputs_combined = torch.cat(rotated_images)
            labels_combined = torch.cat(labels).to(device)
            inputs_combined = inputs_combined.to(device)

            optimizer.zero_grad()
            outputs = model(inputs_combined)
            loss = criterion(outputs, labels_combined)
            loss.backward()
            optimizer.step()

# 3. 異常判定（推論）
def detect_anomaly(model, image):
    model.eval()
    with torch.no_grad():
        # テスト画像を4回回転させて入力し、それぞれのSoftmax確率を取得
        probs = []
        for angle in [0, 90, 180, 270]:
            img_t = TF.rotate(image, angle).unsqueeze(0).to(device)
            output = model(img_t)
            prob = torch.softmax(output, dim=1)
            # 正解の回転角度に対する確率を取り出す
            probs.append(prob[0, angle//90].item())
        
        # 4つの予測の平均確信度を計算。低いほど「異常」
        anomaly_score = 1.0 - np.mean(probs)
    return anomaly_score

```

---

### 3. この手法のメリットと限界

* **メリット**:
* **異常データが一切不要**: 正常画像さえあれば、強力な特徴抽出器を自前で作れる。
* **意味的な理解**: 「ネジは縦に並んでいるのが正常」といった、幾何学的なルールをモデルが学習する。


* **デメリット**:
* **回転不変な物体に弱い**: 丸いコインや、どの向きでも同じに見える液体などの異常検知には向きません（何度回しても予測できないため）。
* **計算コスト**: 学習時に画像を4倍に増やすため、学習に時間がかかる。



---

### 4. Deep RXとの組み合わせ

実は、この **Rotation Prediction で学習したモデルの「途中の層」を Deep RX の特徴抽出器として使う** のが非常に強力なハックです。

1. Rotation Prediction で「その製品専用の目」を作る。
2. そのモデルの Layer 3 などから特徴マップを取り出す。
3. その特徴マップに対して **RX Detector（マハラノビス距離）** を適用する。

これにより、一般の ImageNet（犬や猫）で学習したモデルよりも、あなたの扱う特定の製品（基板や金属部品など）の異常に特化した検知が可能になります。

